{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Quevedo \u00b6 Quevedo is a python tool for creating, annotating and managing datasets of graphical languages, with a focus on the training and evaluation of machine learning algorithms for their recognition. Quevedo is part of the VisSE project . The code can be found at GitHub , and detailed documentation here . Features \u00b6 Dataset management, including hierarchical dataset organization, subset partitioning, and semantically guided data augmentation. Structural annotation of source images using a web interface, with support for different users and the live visualization of data processing scripts. Deep learning network management, training, configuration and evaluation, using darknet . Installation \u00b6 Quevedo requires python >= 3.7 , and can be installed from PyPI : $ pip install quevedo Or, if you want any extras, like the web interface: $ pip install quevedo[web] Or directly from the wheel in the release file : $ pip install quevedo-{version}-py3-none-any.whl[web] You can test that quevedo is working To use the neural network module, you will also need to install darknet . Usage \u00b6 To create a dataset: $ quevedo -D path/to/new/dataset create Then you can cd into the dataset directory so that the -D option is not needed. You can also download an example dataset from this repository ( examples/toy_arithmetic ), or peruse our Corpus of Spanish Signwriting . To see information about a dataset: $ quevedo info To launch the web interface (you must have installed the \"web\" extra): $ quevedo web For more information, and the list of commands, run quevedo --help or quevedo <command> --help or see here . Development \u00b6 To develop on quevedo, we use poetry as our environment, dependency and build management tool. In the quevedo code directory, run: $ poetry install Then you can run quevedo with $ poetry run quevedo Dependencies \u00b6 Quevedo makes use of the following open source projects: python 3 poetry darknet click flask preactjs Additionally, we use the toml and forcelayout libraries, and build our documentation with mkdocs . About \u00b6 Quevedo is licensed under the Open Software License version 3.0 . The web interface includes a copy of preactjs for ease of offline use, distributed under the MIT License . Quevedo is part of the project \"Visualizando la SignoEscritura\" (Proyecto VisSE, Facultad de Inform\u00e1tica, Universidad Complutense de Madrid) as part of the program for funding of research projects on Accesible Technologies financed by INDRA and Fundaci\u00f3n Universia. An expert system developed using Quevedo is described in this article . VisSE team \u00b6 Antonio F. G. Sevilla afgs@ucm.es Alberto D\u00edaz Esteban Jose Mar\u00eda Lahoz-Bengoechea","title":"Home"},{"location":"#quevedo","text":"Quevedo is a python tool for creating, annotating and managing datasets of graphical languages, with a focus on the training and evaluation of machine learning algorithms for their recognition. Quevedo is part of the VisSE project . The code can be found at GitHub , and detailed documentation here .","title":"Quevedo"},{"location":"#features","text":"Dataset management, including hierarchical dataset organization, subset partitioning, and semantically guided data augmentation. Structural annotation of source images using a web interface, with support for different users and the live visualization of data processing scripts. Deep learning network management, training, configuration and evaluation, using darknet .","title":"Features"},{"location":"#installation","text":"Quevedo requires python >= 3.7 , and can be installed from PyPI : $ pip install quevedo Or, if you want any extras, like the web interface: $ pip install quevedo[web] Or directly from the wheel in the release file : $ pip install quevedo-{version}-py3-none-any.whl[web] You can test that quevedo is working To use the neural network module, you will also need to install darknet .","title":"Installation"},{"location":"#usage","text":"To create a dataset: $ quevedo -D path/to/new/dataset create Then you can cd into the dataset directory so that the -D option is not needed. You can also download an example dataset from this repository ( examples/toy_arithmetic ), or peruse our Corpus of Spanish Signwriting . To see information about a dataset: $ quevedo info To launch the web interface (you must have installed the \"web\" extra): $ quevedo web For more information, and the list of commands, run quevedo --help or quevedo <command> --help or see here .","title":"Usage"},{"location":"#development","text":"To develop on quevedo, we use poetry as our environment, dependency and build management tool. In the quevedo code directory, run: $ poetry install Then you can run quevedo with $ poetry run quevedo","title":"Development"},{"location":"#dependencies","text":"Quevedo makes use of the following open source projects: python 3 poetry darknet click flask preactjs Additionally, we use the toml and forcelayout libraries, and build our documentation with mkdocs .","title":"Dependencies"},{"location":"#about","text":"Quevedo is licensed under the Open Software License version 3.0 . The web interface includes a copy of preactjs for ease of offline use, distributed under the MIT License . Quevedo is part of the project \"Visualizando la SignoEscritura\" (Proyecto VisSE, Facultad de Inform\u00e1tica, Universidad Complutense de Madrid) as part of the program for funding of research projects on Accesible Technologies financed by INDRA and Fundaci\u00f3n Universia. An expert system developed using Quevedo is described in this article .","title":"About"},{"location":"#visse-team","text":"Antonio F. G. Sevilla afgs@ucm.es Alberto D\u00edaz Esteban Jose Mar\u00eda Lahoz-Bengoechea","title":"VisSE team"},{"location":"api/","text":"Reference \u00b6 Datasets \u00b6 Dataset objects are the main entry point for user code in Quevedo. They provide methods to manage the dataset, but also to retrieve other objects within it. Therefore, you don't usually need to create instances of other objects directly, but rather use the methods in the Dataset class to get them already built. For example: from quevedo import Dataset, Target ds = Dataset('path/to/the/dataset') # annotation is of type quevedo.Grapheme, a subclass of quevedo.Annotation annotation = ds.get_single(Target.GRAPH, 'subset', 32) print(annotation.to_dict()) # net is of type quevedo.Network net = ds.get_network('grapheme_classify') net.auto_annotate(annotation) annotation.save() # creating a quevedo.Logogram (subclass of quevedo.Annotation) new_a = ds.new_single(Target.LOGO, 'my_new_subset', image_path='path/to/the/image', graphemes=[ {'tags': ['character', 'letter_a'], 'box': [0.2, 0.6, 0.3, 0.3]}, {'tags': ['character', 'accute_accent'] 'box': [0.2, 0.2, 0.1, 0.2]} ]) class Dataset ( path='.' ) Class representing a Quevedo dataset. It provides access to the annotations, subsets, and any neural networks contained. Parameters path (optional) \u2014 the path to the dataset directory (existing or to be created) Attributes config (dict) \u2014 Dataset configuration path (pathlib.Path) \u2014 Path to the dataset directory. Methods create ( ) \u2014 Create or initialize a directory to be a Quevedo dataset. create_subset ( target , name , existing ) \u2014 Creates the directory for a new subset. get_annotations ( target , subset ) \u2014 Get annotations from the dataset. get_config ( section , key ) \u2014 Get the configuration for a key under a section (a value in a table, eg [network.example], where network is the section and example is the key. This method looks for the \"extend\" key and merges configuration recursively. get_network ( name ) \u2014 Get a single neural network by name. get_pipeline ( name ) \u2014 Get a pipeline by name. get_single ( target , subset , id ) \u2014 Retrieve a single annotation. get_subsets ( target ) \u2014 Gets information about subsets in the dataset. is_test ( annotation ) \u2014 Checks if an annotation belongs to the training split. is_train ( annotation ) \u2014 Checks if an annotation belongs to the training split. list_networks ( ) \u2014 Get a list of all neural networks for this dataset. list_pipelines ( ) \u2014 Get a list of all pipelines for this dataset. new_single ( target , subset , **kwds ) \u2014 Create a new annotation. method get_config ( section , key ) Get the configuration for a key under a section (a value in a table, eg [network.example], where network is the section and example is the key. This method looks for the \"extend\" key and merges configuration recursively. Returns dict method create ( ) Create or initialize a directory to be a Quevedo dataset. method list_networks ( ) Get a list of all neural networks for this dataset. Returns list of Networks method get_network ( name ) Get a single neural network by name. Parameters name \u2014 name of the neural network as specified in the configuration file. Returns a Network object. method list_pipelines ( ) Get a list of all pipelines for this dataset. Returns list of Pipelines method get_pipeline ( name ) Get a pipeline by name. Parameters name \u2014 name of the pipeline as specified in the configuration file. Returns a Pipeline object. method get_single ( target , subset , id ) Retrieve a single annotation. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation to retrieve. subset \u2014 name of the subset where the annotation is stored. id \u2014 number of the annotation in the subset. Returns a single Annotation of the appropriate type. method new_single ( target , subset , **kwds ) Create a new annotation. This method creates the annotation files in the corresponding directory, and initializes them with create_from . Any extra arguments will be passed to that method. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation to create. subset \u2014 name of the (existing) subset where to place it. Returns the new Annotation . method get_annotations ( target=<AnnotationTarget.GRAPH|LOGO: 3> , subset=None ) Get annotations from the dataset. Depending on the arguments, all annotations, those of a given target, or only those in a given subset (or subsets) and target will be selected. Parameters target (AnnotationTarget, optional) \u2014 Target (type) of the annotations to retrieve. By default, it is the union of both types, so all annotations are retrieved: Target.GRAPH | Target.LOGO . subset (optional) \u2014 name of the subsets to get, or None to get annotations from all subsets. Returns a generator that yields selected annotations. method get_subsets ( target ) Gets information about subsets in the dataset. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation subsets. Returns a sorted list of dict , each with the keys name for the name of the subset, and count for the number of annotations in it. method create_subset ( target , name , existing='a' ) Creates the directory for a new subset. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation subset to create. name \u2014 name for the new subset. existing (optional) \u2014 controls behaviour when the directory already exists. It can be 'a' to abort (the default), 'r' to remove existing annotations, or 'm' (merge) to do nothing. Returns the path of the created directory. method is_train ( annotation ) Checks if an annotation belongs to the training split. method is_test ( annotation ) Checks if an annotation belongs to the training split. Annotations \u00b6 Quevedo annotations are of two types, logograms and graphemes, both derived from the parent class Annotation . When it is necessary to distinguish logograms and graphemes in a process, there is the enum Target , which can take the values Target.GRAPH or Target.LOGO . the values Target.GRAPH or Target.LOGO . There is also the BoundGrapheme class, used to represent each of the graphemes which make up a logogram. class Annotation ( path=None , image=None , **kwds ) Class representing a single annotation of either a logogram of a sign or signs in the dataset or an isolated grapheme. Parameters path (optional) \u2014 the full path to the annotation files (either source image or tag dictionary, which should share path and filename but not extension (the annotation dictionary need not exist). image (optional) \u2014 either a file object or a PIL image to create a \"path-less\" annotation which lives in memory. Attributes id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for this annotation. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. meta \u2014 Dictionary of metadata annotations. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( meta , fold , **kwds ) \u2014 Update the content of the annotation. method update ( meta=None , fold=None , **kwds ) Update the content of the annotation. This method should be overriden by the specific annotation classes to add their specific annotation information. Parameters meta (optional) \u2014 dictionary of metadata values to set. fold (optional) \u2014 fold to which the annotation will belong. method to_dict ( ) Get the annotation data as a dictionary. method save ( ) Persist the information to the filesystem. method create_from ( image_path=None , binary_data=None , pil_image=None , **kwds ) Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. class Grapheme ( *args , **kwargs ) Bases quevedo.annotation.annotation.Annotation Annotation for an isolated grapheme. Attributes id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for this annotation. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. meta \u2014 Dictionary of metadata annotations. tags \u2014 annotated tags for this grapheme. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( tags , **kwds ) \u2014 Extends base update , other arguments will be passed through. method save ( ) Persist the information to the filesystem. method create_from ( image_path=None , binary_data=None , pil_image=None , **kwds ) Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. method update ( tags=None , **kwds ) Extends base update , other arguments will be passed through. Parameters tags (optional) \u2014 new tags for this grapheme (replaces all). method to_dict ( ) Get the annotation data as a dictionary. class Logogram ( *args , **kwargs ) Bases quevedo.annotation.annotation.Annotation Annotation for a logogram, with its contained graphemes. Attributes edges \u2014 list of edges found within this logogram. graphemes \u2014 list of bound graphemes found within this logogram. id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for this annotation. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. meta \u2014 Dictionary of metadata annotations. tags \u2014 annotated tags for this logogram. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( tags , graphemes , edges , **kwds ) \u2014 Extends base update , other arguments will be passed through. method save ( ) Persist the information to the filesystem. method create_from ( image_path=None , binary_data=None , pil_image=None , **kwds ) Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. method update ( tags=None , graphemes=None , edges=None , **kwds ) Extends base update , other arguments will be passed through. Parameters tags (optional) \u2014 new tags for this logogram (replaces all). graphemes (optional) \u2014 either a list of Graphemes, BoundGraphemes, or dicts with the keys necessary to initialize a BoundGrapheme . edges (optional) \u2014 either a list of Edges, or dicts with the keys necessary to initialize an Edge . In this case, start and end should be the indices of the boundgraphemes in the graphemes list. method to_dict ( ) Get the annotation data as a dictionary. class BoundGrapheme ( logogram , box=[0, 0, 0, 0] , *args , **kwargs ) Bases quevedo.annotation.grapheme.Grapheme quevedo.annotation.annotation.Annotation A grapheme which is not isolated, but rather forms part of a logogram. To promote this bound grapheme to an isolated grapheme with its own annotation, create a grapheme object using create_from , passing this object's image to the argument pil_image . Attributes box (list[float]) \u2014 Bounding box coordinates (x, y, w, h) of this grapheme within the logogram. (x, y): coordinates of the center . (w, h): width and height. Values are relative to the logogram size, in the range [0, 1] . id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for only this grapheme, cropped out of the parent logogram's image. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. logogram \u2014 Logogram where this grapheme is found. meta \u2014 Dictionary of metadata annotations. tags \u2014 annotated tags for this grapheme. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. inbound ( ) \u2014 Generator[Edge,None,None]: edges in the logogram ending in this grapheme. outbound ( ) \u2014 Generator[Edge,None,None]: edges in the logogram emanating from this grapheme. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( tags , **kwds ) \u2014 Extends base update , other arguments will be passed through. method save ( ) Persist the information to the filesystem. method create_from ( image_path=None , binary_data=None , pil_image=None , **kwds ) Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. method update ( tags=None , **kwds ) Extends base update , other arguments will be passed through. Parameters tags (optional) \u2014 new tags for this grapheme (replaces all). method to_dict ( ) Get the annotation data as a dictionary. method outbound ( ) Generator[Edge,None,None]: edges in the logogram emanating from this grapheme. method inbound ( ) Generator[Edge,None,None]: edges in the logogram ending in this grapheme. class Edge ( start , end , tags={} ) An edge between graphemes in a logogram. Edges are used to connect two graphemes, and can be used to define the dependency or function between them. The edges and graphemes of a logogram form a directed graph. The tags for an edge are a dictionary with keys in the dataset's e_tags field. Attributes end \u2014 bound grapheme end of the edge start \u2014 bound grapheme origin of the edge tags \u2014 annotated tags for this edge. Networks \u00b6 Network objects in Quevedo represent the network itself, but also their configuration, training and testing process, and use. There are two types of networks, Detector networks and Classifier networks, which work on logograms and graphemes respectively. The Network base class documented here is a base class that defines general behaviour, but code specific to each type of network lives in its own class. Therefore, you should get the network from a Quevedo dataset's method get_network so that the proper instance is built. class Network ( dataset , name , config ) Class representing a neural net to train and predict logograms or graphemes. Attributes config \u2014 Configuration dictionary dataset \u2014 Parent dataset get_tag \u2014 function to get the relevant label for the network from a list of tags according to g_tags name \u2014 Name of the network path \u2014 Path to the network directory prediction_to_tag \u2014 function to get the g_tags values from the tag/label/class predicted by the network Methods auto_annotate ( annotation ) \u2014 Use the network to automatically annotate a real instance. get_annotations ( test ) \u2014 Get the annotations configured for use with this network. is_prepared ( ) \u2014 Checks whether the neural network configuration files have been made. is_trained ( ) \u2014 Checks whether the neural network has been trained and can be used to predict. predict ( image_path ) \u2014 Use the trained neural network to predict results from an image. prepare ( ) \u2014 Creates the files needed for training (and later using) darknet. test ( annotation , stats ) \u2014 Method to test the network on an annotation. train ( initial ) \u2014 Trains the neural network. method is_prepared ( ) Checks whether the neural network configuration files have been made. method is_trained ( ) Checks whether the neural network has been trained and can be used to predict. method get_annotations ( test=False ) Get the annotations configured for use with this network. Parameters test (optional) \u2014 get test annotations instead of train Returns a list of relevant Annotations . method prepare ( ) Creates the files needed for training (and later using) darknet. Stores the files in the network directory so they can be reused or tracked by a version control system. Must be called before training, and files not deleted (except maybe the \"train\" directory) before testing or predicting with the net. method train ( initial=None ) Trains the neural network. When finished, removes partial weights and keeps only the last. Can be interrupted and optionally resumed later. Parameters initial (optional) \u2014 path to the weights from which to resume training. method predict ( image_path ) Use the trained neural network to predict results from an image. Parameters image_path \u2014 path to the image in the file system. Returns a list of dictionaries with the predictions. Each prediction has a confidence value. Classify networks have a tag key for the predicted class, each entry is a possible classification of the image. Detector networks results are possible graphemes found, each with a name (predicted class) and box (bounding box). method test ( annotation , stats ) Method to test the network on an annotation. This method is intended for internal library use, you probably want to use predict instead. Uses the network to get the prediction for a real annotation, compare results and update stats. See test.py for stats . method auto_annotate ( annotation ) Use the network to automatically annotate a real instance. In detector networks, existing bound graphemes will be removed. In classify networks, tags which are not relevant to this network won't be modified, so it can be used incrementally. Parameters annotation \u2014 Annotation to automatically tag using this network's predictions. Pipelines \u00b6 Pipeline objects in Quevedo can often act as substitutes for networks, to perform their task but using a more complex system of networks and steps. There are a number of types of pipelines, each represented by a different class in the code, and a base class that serves as common interface. To get a configured pipeline from a dataset, use its get_pipeline method. class Pipeline ( dataset , name , config ) A pipeline combines networks and logical steps into a computational graph that can be used for the task of detection or classification of logograms or graphemes. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. staticmethod run ( a ) Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. method predict ( image_path ) Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. class NetworkPipeline ( dataset , name , config ) Bases quevedo.pipeline.Pipeline A pipeline step that runs a network on the given annotation. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method predict ( image_path ) Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method run ( a ) Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class LogogramPipeline ( dataset , name , config ) Bases quevedo.pipeline.Pipeline A pipeline for detecting graphemes within a logogram and then classifying them, using networks or sub pipelines. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method predict ( image_path ) Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method run ( a ) Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class SequencePipeline ( dataset , name , config ) Bases quevedo.pipeline.Pipeline A pipeline that runs a sequence of other pipelines. All steps should have the same target. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method predict ( image_path ) Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method run ( a ) Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class BranchPipeline ( dataset , name , config ) Bases quevedo.pipeline.Pipeline A pipeline that runs one of many possible branches depending on a criterion. The criterion can be: a tag name: the pipeline will run the branch corresponding to the tag value. Can also be a meta tag. a lambda expression: the pipeline will run the branch corresponding to the result of the lambda expression, which will receive the annotation as parameter. All branches should have the same target. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method predict ( image_path ) Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method run ( a ) Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class FunctionPipeline ( dataset , name , config ) Bases quevedo.pipeline.Pipeline A pipeline that runs a user-defined function. The config should be a string in the form 'module.py:function'. 'module.py' should be a file in the scripts directory of the dataset, and 'function' should be the name of a function in that file, that accepts an annotation and a dataset and returns nothing. Alternatively, a string containing a lambda function, receiving the same arguments, can be used. The target of this pipeline is deduced from the signature of the function. This is often inconsequential, but if this network is the first of a sequence or branching, its target will be the target for the whole pipeline. To ensure correct deduction, use a type annotation of Logogram or Grapheme for the second parameter. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method predict ( image_path ) Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method run ( a ) Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on.","title":"Reference"},{"location":"api/#reference","text":"","title":"Reference"},{"location":"api/#datasets","text":"Dataset objects are the main entry point for user code in Quevedo. They provide methods to manage the dataset, but also to retrieve other objects within it. Therefore, you don't usually need to create instances of other objects directly, but rather use the methods in the Dataset class to get them already built. For example: from quevedo import Dataset, Target ds = Dataset('path/to/the/dataset') # annotation is of type quevedo.Grapheme, a subclass of quevedo.Annotation annotation = ds.get_single(Target.GRAPH, 'subset', 32) print(annotation.to_dict()) # net is of type quevedo.Network net = ds.get_network('grapheme_classify') net.auto_annotate(annotation) annotation.save() # creating a quevedo.Logogram (subclass of quevedo.Annotation) new_a = ds.new_single(Target.LOGO, 'my_new_subset', image_path='path/to/the/image', graphemes=[ {'tags': ['character', 'letter_a'], 'box': [0.2, 0.6, 0.3, 0.3]}, {'tags': ['character', 'accute_accent'] 'box': [0.2, 0.2, 0.1, 0.2]} ]) class","title":"Datasets"},{"location":"api/#quevedodatasetdataset","text":"Class representing a Quevedo dataset. It provides access to the annotations, subsets, and any neural networks contained. Parameters path (optional) \u2014 the path to the dataset directory (existing or to be created) Attributes config (dict) \u2014 Dataset configuration path (pathlib.Path) \u2014 Path to the dataset directory. Methods create ( ) \u2014 Create or initialize a directory to be a Quevedo dataset. create_subset ( target , name , existing ) \u2014 Creates the directory for a new subset. get_annotations ( target , subset ) \u2014 Get annotations from the dataset. get_config ( section , key ) \u2014 Get the configuration for a key under a section (a value in a table, eg [network.example], where network is the section and example is the key. This method looks for the \"extend\" key and merges configuration recursively. get_network ( name ) \u2014 Get a single neural network by name. get_pipeline ( name ) \u2014 Get a pipeline by name. get_single ( target , subset , id ) \u2014 Retrieve a single annotation. get_subsets ( target ) \u2014 Gets information about subsets in the dataset. is_test ( annotation ) \u2014 Checks if an annotation belongs to the training split. is_train ( annotation ) \u2014 Checks if an annotation belongs to the training split. list_networks ( ) \u2014 Get a list of all neural networks for this dataset. list_pipelines ( ) \u2014 Get a list of all pipelines for this dataset. new_single ( target , subset , **kwds ) \u2014 Create a new annotation. method","title":"Dataset"},{"location":"api/#quevedodatasetdatasetget_config","text":"Get the configuration for a key under a section (a value in a table, eg [network.example], where network is the section and example is the key. This method looks for the \"extend\" key and merges configuration recursively. Returns dict method","title":"get_config"},{"location":"api/#quevedodatasetdatasetcreate","text":"Create or initialize a directory to be a Quevedo dataset. method","title":"create"},{"location":"api/#quevedodatasetdatasetlist_networks","text":"Get a list of all neural networks for this dataset. Returns list of Networks method","title":"list_networks"},{"location":"api/#quevedodatasetdatasetget_network","text":"Get a single neural network by name. Parameters name \u2014 name of the neural network as specified in the configuration file. Returns a Network object. method","title":"get_network"},{"location":"api/#quevedodatasetdatasetlist_pipelines","text":"Get a list of all pipelines for this dataset. Returns list of Pipelines method","title":"list_pipelines"},{"location":"api/#quevedodatasetdatasetget_pipeline","text":"Get a pipeline by name. Parameters name \u2014 name of the pipeline as specified in the configuration file. Returns a Pipeline object. method","title":"get_pipeline"},{"location":"api/#quevedodatasetdatasetget_single","text":"Retrieve a single annotation. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation to retrieve. subset \u2014 name of the subset where the annotation is stored. id \u2014 number of the annotation in the subset. Returns a single Annotation of the appropriate type. method","title":"get_single"},{"location":"api/#quevedodatasetdatasetnew_single","text":"Create a new annotation. This method creates the annotation files in the corresponding directory, and initializes them with create_from . Any extra arguments will be passed to that method. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation to create. subset \u2014 name of the (existing) subset where to place it. Returns the new Annotation . method","title":"new_single"},{"location":"api/#quevedodatasetdatasetget_annotations","text":"Get annotations from the dataset. Depending on the arguments, all annotations, those of a given target, or only those in a given subset (or subsets) and target will be selected. Parameters target (AnnotationTarget, optional) \u2014 Target (type) of the annotations to retrieve. By default, it is the union of both types, so all annotations are retrieved: Target.GRAPH | Target.LOGO . subset (optional) \u2014 name of the subsets to get, or None to get annotations from all subsets. Returns a generator that yields selected annotations. method","title":"get_annotations"},{"location":"api/#quevedodatasetdatasetget_subsets","text":"Gets information about subsets in the dataset. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation subsets. Returns a sorted list of dict , each with the keys name for the name of the subset, and count for the number of annotations in it. method","title":"get_subsets"},{"location":"api/#quevedodatasetdatasetcreate_subset","text":"Creates the directory for a new subset. Parameters target (AnnotationTarget) \u2014 Target (type) of the annotation subset to create. name \u2014 name for the new subset. existing (optional) \u2014 controls behaviour when the directory already exists. It can be 'a' to abort (the default), 'r' to remove existing annotations, or 'm' (merge) to do nothing. Returns the path of the created directory. method","title":"create_subset"},{"location":"api/#quevedodatasetdatasetis_train","text":"Checks if an annotation belongs to the training split. method","title":"is_train"},{"location":"api/#quevedodatasetdatasetis_test","text":"Checks if an annotation belongs to the training split.","title":"is_test"},{"location":"api/#annotations","text":"Quevedo annotations are of two types, logograms and graphemes, both derived from the parent class Annotation . When it is necessary to distinguish logograms and graphemes in a process, there is the enum Target , which can take the values Target.GRAPH or Target.LOGO . the values Target.GRAPH or Target.LOGO . There is also the BoundGrapheme class, used to represent each of the graphemes which make up a logogram. class","title":"Annotations"},{"location":"api/#quevedoannotationannotationannotation","text":"Class representing a single annotation of either a logogram of a sign or signs in the dataset or an isolated grapheme. Parameters path (optional) \u2014 the full path to the annotation files (either source image or tag dictionary, which should share path and filename but not extension (the annotation dictionary need not exist). image (optional) \u2014 either a file object or a PIL image to create a \"path-less\" annotation which lives in memory. Attributes id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for this annotation. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. meta \u2014 Dictionary of metadata annotations. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( meta , fold , **kwds ) \u2014 Update the content of the annotation. method","title":"Annotation"},{"location":"api/#quevedoannotationannotationannotationupdate","text":"Update the content of the annotation. This method should be overriden by the specific annotation classes to add their specific annotation information. Parameters meta (optional) \u2014 dictionary of metadata values to set. fold (optional) \u2014 fold to which the annotation will belong. method","title":"update"},{"location":"api/#quevedoannotationannotationannotationto_dict","text":"Get the annotation data as a dictionary. method","title":"to_dict"},{"location":"api/#quevedoannotationannotationannotationsave","text":"Persist the information to the filesystem. method","title":"save"},{"location":"api/#quevedoannotationannotationannotationcreate_from","text":"Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. class","title":"create_from"},{"location":"api/#quevedoannotationgraphemegrapheme","text":"Bases quevedo.annotation.annotation.Annotation Annotation for an isolated grapheme. Attributes id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for this annotation. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. meta \u2014 Dictionary of metadata annotations. tags \u2014 annotated tags for this grapheme. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( tags , **kwds ) \u2014 Extends base update , other arguments will be passed through. method","title":"Grapheme"},{"location":"api/#quevedoannotationannotationannotationsave_1","text":"Persist the information to the filesystem. method","title":"save"},{"location":"api/#quevedoannotationannotationannotationcreate_from_1","text":"Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. method","title":"create_from"},{"location":"api/#quevedoannotationgraphemegraphemeupdate","text":"Extends base update , other arguments will be passed through. Parameters tags (optional) \u2014 new tags for this grapheme (replaces all). method","title":"update"},{"location":"api/#quevedoannotationgraphemegraphemeto_dict","text":"Get the annotation data as a dictionary. class","title":"to_dict"},{"location":"api/#quevedoannotationlogogramlogogram","text":"Bases quevedo.annotation.annotation.Annotation Annotation for a logogram, with its contained graphemes. Attributes edges \u2014 list of edges found within this logogram. graphemes \u2014 list of bound graphemes found within this logogram. id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for this annotation. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. meta \u2014 Dictionary of metadata annotations. tags \u2014 annotated tags for this logogram. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( tags , graphemes , edges , **kwds ) \u2014 Extends base update , other arguments will be passed through. method","title":"Logogram"},{"location":"api/#quevedoannotationannotationannotationsave_2","text":"Persist the information to the filesystem. method","title":"save"},{"location":"api/#quevedoannotationannotationannotationcreate_from_2","text":"Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. method","title":"create_from"},{"location":"api/#quevedoannotationlogogramlogogramupdate","text":"Extends base update , other arguments will be passed through. Parameters tags (optional) \u2014 new tags for this logogram (replaces all). graphemes (optional) \u2014 either a list of Graphemes, BoundGraphemes, or dicts with the keys necessary to initialize a BoundGrapheme . edges (optional) \u2014 either a list of Edges, or dicts with the keys necessary to initialize an Edge . In this case, start and end should be the indices of the boundgraphemes in the graphemes list. method","title":"update"},{"location":"api/#quevedoannotationlogogramlogogramto_dict","text":"Get the annotation data as a dictionary. class","title":"to_dict"},{"location":"api/#quevedoannotationlogogramboundgrapheme","text":"Bases quevedo.annotation.grapheme.Grapheme quevedo.annotation.annotation.Annotation A grapheme which is not isolated, but rather forms part of a logogram. To promote this bound grapheme to an isolated grapheme with its own annotation, create a grapheme object using create_from , passing this object's image to the argument pil_image . Attributes box (list[float]) \u2014 Bounding box coordinates (x, y, w, h) of this grapheme within the logogram. (x, y): coordinates of the center . (w, h): width and height. Values are relative to the logogram size, in the range [0, 1] . id \u2014 Number which identifies this annotation in its subset. image (PIL.Image.Image) \u2014 image data for only this grapheme, cropped out of the parent logogram's image. image_path \u2014 Path to the source image for the annotation. It is the id plus png extension. json_path \u2014 Path to the json annotation file. It is the id plus json extension. logogram \u2014 Logogram where this grapheme is found. meta \u2014 Dictionary of metadata annotations. tags \u2014 annotated tags for this grapheme. Methods create_from ( image_path , binary_data , pil_image , **kwds ) \u2014 Initialize an annotation with some source image data. inbound ( ) \u2014 Generator[Edge,None,None]: edges in the logogram ending in this grapheme. outbound ( ) \u2014 Generator[Edge,None,None]: edges in the logogram emanating from this grapheme. save ( ) \u2014 Persist the information to the filesystem. to_dict ( ) \u2014 Get the annotation data as a dictionary. update ( tags , **kwds ) \u2014 Extends base update , other arguments will be passed through. method","title":"BoundGrapheme"},{"location":"api/#quevedoannotationannotationannotationsave_3","text":"Persist the information to the filesystem. method","title":"save"},{"location":"api/#quevedoannotationannotationannotationcreate_from_3","text":"Initialize an annotation with some source image data. One of image_path , binary_data or pil_image must be provided. Other arguments will be passed to update so metadata or tags can also be set. The annotation will be persisted with a call to save too. Parameters image_path (optional) \u2014 path to an image in the filesystem to use as image for this annotation. The image will be copied into the dataset. binary_data (optional) \u2014 bytes array which encodes the source image. The contents will be dumped to the appropriate image file in the dataset. pil_image (optional) \u2014 [PIL.Image.Image] object to be stored as image file for this annotation in the dataset. Returns self to allow chaining. method","title":"create_from"},{"location":"api/#quevedoannotationgraphemegraphemeupdate_1","text":"Extends base update , other arguments will be passed through. Parameters tags (optional) \u2014 new tags for this grapheme (replaces all). method","title":"update"},{"location":"api/#quevedoannotationlogogramboundgraphemeto_dict","text":"Get the annotation data as a dictionary. method","title":"to_dict"},{"location":"api/#quevedoannotationlogogramboundgraphemeoutbound","text":"Generator[Edge,None,None]: edges in the logogram emanating from this grapheme. method","title":"outbound"},{"location":"api/#quevedoannotationlogogramboundgraphemeinbound","text":"Generator[Edge,None,None]: edges in the logogram ending in this grapheme. class","title":"inbound"},{"location":"api/#quevedoannotationlogogramedge","text":"An edge between graphemes in a logogram. Edges are used to connect two graphemes, and can be used to define the dependency or function between them. The edges and graphemes of a logogram form a directed graph. The tags for an edge are a dictionary with keys in the dataset's e_tags field. Attributes end \u2014 bound grapheme end of the edge start \u2014 bound grapheme origin of the edge tags \u2014 annotated tags for this edge.","title":"Edge"},{"location":"api/#networks","text":"Network objects in Quevedo represent the network itself, but also their configuration, training and testing process, and use. There are two types of networks, Detector networks and Classifier networks, which work on logograms and graphemes respectively. The Network base class documented here is a base class that defines general behaviour, but code specific to each type of network lives in its own class. Therefore, you should get the network from a Quevedo dataset's method get_network so that the proper instance is built. class","title":"Networks"},{"location":"api/#quevedonetworknetworknetwork","text":"Class representing a neural net to train and predict logograms or graphemes. Attributes config \u2014 Configuration dictionary dataset \u2014 Parent dataset get_tag \u2014 function to get the relevant label for the network from a list of tags according to g_tags name \u2014 Name of the network path \u2014 Path to the network directory prediction_to_tag \u2014 function to get the g_tags values from the tag/label/class predicted by the network Methods auto_annotate ( annotation ) \u2014 Use the network to automatically annotate a real instance. get_annotations ( test ) \u2014 Get the annotations configured for use with this network. is_prepared ( ) \u2014 Checks whether the neural network configuration files have been made. is_trained ( ) \u2014 Checks whether the neural network has been trained and can be used to predict. predict ( image_path ) \u2014 Use the trained neural network to predict results from an image. prepare ( ) \u2014 Creates the files needed for training (and later using) darknet. test ( annotation , stats ) \u2014 Method to test the network on an annotation. train ( initial ) \u2014 Trains the neural network. method","title":"Network"},{"location":"api/#quevedonetworknetworknetworkis_prepared","text":"Checks whether the neural network configuration files have been made. method","title":"is_prepared"},{"location":"api/#quevedonetworknetworknetworkis_trained","text":"Checks whether the neural network has been trained and can be used to predict. method","title":"is_trained"},{"location":"api/#quevedonetworknetworknetworkget_annotations","text":"Get the annotations configured for use with this network. Parameters test (optional) \u2014 get test annotations instead of train Returns a list of relevant Annotations . method","title":"get_annotations"},{"location":"api/#quevedonetworknetworknetworkprepare","text":"Creates the files needed for training (and later using) darknet. Stores the files in the network directory so they can be reused or tracked by a version control system. Must be called before training, and files not deleted (except maybe the \"train\" directory) before testing or predicting with the net. method","title":"prepare"},{"location":"api/#quevedonetworknetworknetworktrain","text":"Trains the neural network. When finished, removes partial weights and keeps only the last. Can be interrupted and optionally resumed later. Parameters initial (optional) \u2014 path to the weights from which to resume training. method","title":"train"},{"location":"api/#quevedonetworknetworknetworkpredict","text":"Use the trained neural network to predict results from an image. Parameters image_path \u2014 path to the image in the file system. Returns a list of dictionaries with the predictions. Each prediction has a confidence value. Classify networks have a tag key for the predicted class, each entry is a possible classification of the image. Detector networks results are possible graphemes found, each with a name (predicted class) and box (bounding box). method","title":"predict"},{"location":"api/#quevedonetworknetworknetworktest","text":"Method to test the network on an annotation. This method is intended for internal library use, you probably want to use predict instead. Uses the network to get the prediction for a real annotation, compare results and update stats. See test.py for stats . method","title":"test"},{"location":"api/#quevedonetworknetworknetworkauto_annotate","text":"Use the network to automatically annotate a real instance. In detector networks, existing bound graphemes will be removed. In classify networks, tags which are not relevant to this network won't be modified, so it can be used incrementally. Parameters annotation \u2014 Annotation to automatically tag using this network's predictions.","title":"auto_annotate"},{"location":"api/#pipelines","text":"Pipeline objects in Quevedo can often act as substitutes for networks, to perform their task but using a more complex system of networks and steps. There are a number of types of pipelines, each represented by a different class in the code, and a base class that serves as common interface. To get a configured pipeline from a dataset, use its get_pipeline method. class","title":"Pipelines"},{"location":"api/#quevedopipelinepipeline","text":"A pipeline combines networks and logical steps into a computational graph that can be used for the task of detection or classification of logograms or graphemes. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. staticmethod","title":"Pipeline"},{"location":"api/#quevedopipelinepipelinerun","text":"Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. method","title":"run"},{"location":"api/#quevedopipelinepipelinepredict","text":"Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. class","title":"predict"},{"location":"api/#quevedopipelinenetworkpipeline","text":"Bases quevedo.pipeline.Pipeline A pipeline step that runs a network on the given annotation. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method","title":"NetworkPipeline"},{"location":"api/#quevedopipelinepipelinepredict_1","text":"Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method","title":"predict"},{"location":"api/#quevedopipelinenetworkpipelinerun","text":"Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class","title":"run"},{"location":"api/#quevedopipelinelogogrampipeline","text":"Bases quevedo.pipeline.Pipeline A pipeline for detecting graphemes within a logogram and then classifying them, using networks or sub pipelines. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method","title":"LogogramPipeline"},{"location":"api/#quevedopipelinepipelinepredict_2","text":"Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method","title":"predict"},{"location":"api/#quevedopipelinelogogrampipelinerun","text":"Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class","title":"run"},{"location":"api/#quevedopipelinesequencepipeline","text":"Bases quevedo.pipeline.Pipeline A pipeline that runs a sequence of other pipelines. All steps should have the same target. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method","title":"SequencePipeline"},{"location":"api/#quevedopipelinepipelinepredict_3","text":"Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method","title":"predict"},{"location":"api/#quevedopipelinesequencepipelinerun","text":"Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class","title":"run"},{"location":"api/#quevedopipelinebranchpipeline","text":"Bases quevedo.pipeline.Pipeline A pipeline that runs one of many possible branches depending on a criterion. The criterion can be: a tag name: the pipeline will run the branch corresponding to the tag value. Can also be a meta tag. a lambda expression: the pipeline will run the branch corresponding to the result of the lambda expression, which will receive the annotation as parameter. All branches should have the same target. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method","title":"BranchPipeline"},{"location":"api/#quevedopipelinepipelinepredict_4","text":"Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method","title":"predict"},{"location":"api/#quevedopipelinebranchpipelinerun","text":"Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on. class","title":"run"},{"location":"api/#quevedopipelinefunctionpipeline","text":"Bases quevedo.pipeline.Pipeline A pipeline that runs a user-defined function. The config should be a string in the form 'module.py:function'. 'module.py' should be a file in the scripts directory of the dataset, and 'function' should be the name of a function in that file, that accepts an annotation and a dataset and returns nothing. Alternatively, a string containing a lambda function, receiving the same arguments, can be used. The target of this pipeline is deduced from the signature of the function. This is often inconsequential, but if this network is the first of a sequence or branching, its target will be the target for the whole pipeline. To ensure correct deduction, use a type annotation of Logogram or Grapheme for the second parameter. Methods predict ( image_path ) (Annotation) \u2014 Run the pipeline on the given image and return the resulting annotation. run ( a ) \u2014 Run the pipeline on the given annotation. method","title":"FunctionPipeline"},{"location":"api/#quevedopipelinepipelinepredict_5","text":"Run the pipeline on the given image and return the resulting annotation. Parameters image_path (str) \u2014 Path to the image to run the pipeline on. Returns (Annotation) The resulting annotation. method","title":"predict"},{"location":"api/#quevedopipelinefunctionpipelinerun","text":"Run the pipeline on the given annotation. Parameters a (Annotation) \u2014 Annotation to run the pipeline on.","title":"run"},{"location":"cli/","text":"Command Line Interface \u00b6 Usage: quevedo [OPTIONS] COMMAND1 [ARGS]... [COMMAND2 [ARGS]...]... Quevedo is a tool for managing datasets of images with compositional semantics. This includes file management, annotation of data, and neural network training and use. The -D, -N and -P flags are global, and affect all commands used afterwards. -N and -P are exclusive. For example, to run a full experiment for neural network 'one', run: quevedo -D path/to/dataset -N one split -p 80 prepare train test Options: -D, --dataset PATH Path to the dataset to use, by default use current directory. -N, --network TEXT Neural network configuration to use. -P, --pipeline TEXT Pipeline configuration to use. --version Show the version and exit. --help Show this message and exit. Commands: add_images Import images from external directories into the dataset. config Edit dataset configuration. create Create and initialize a Quevedo dataset. extract Extract graphemes from annotated logograms. generate Generate artificial logograms from existing graphemes. info Get general status information about a dataset. migrate Upgrades a dataset config and data to the latest version. predict Get predictions for an image using a trained neural network... prepare Create the files needed for training and using this network. run_script Run a data processing script on dataset objects. split Assign annotations randomly to different folds. test Compute evaluation metrics for a trained neural network or... train Train the neural network. web Run a web interface to the dataset. config \u00b6 Usage: quevedo config [OPTIONS] Edit dataset configuration. This command is a simple convenience to launch an editor open at the configuration file (config.toml). Options: -e, --editor TEXT Editor to use instead of the automatically detected one --help Show this message and exit. info \u00b6 Usage: quevedo info [OPTIONS] Get general status information about a dataset. Options: --help Show this message and exit. create \u00b6 Usage: quevedo create [OPTIONS] Create and initialize a Quevedo dataset. Options: --help Show this message and exit. add_images \u00b6 Usage: quevedo add_images [OPTIONS] Import images from external directories into the dataset. For now, images need to be in the PNG format, and have 3 channels (color) and 8 bit depth. Options: -i, --image_dir PATH Directory from which to import images. [required] -g, --grapheme-set TEXT Import the images to this grapheme set. -l, --logogram-set TEXT Import the images to this logogram set. -m, --merge Merge new images with existing ones, if any. -r, --replace Replace old images with new ones, if any. --sort-numeric Sort images ids by filename (numeric). --sort-alphabetic Sort images ids by filename (alphabetic). --no-sort Don't sort images to import. [default] --help Show this message and exit. split \u00b6 Usage: quevedo split [OPTIONS] Assign annotations randomly to different folds. By default, the annotations will be split into a number of folds configured in the dataset configuration file, starting from 0. To override this, use the `-s` and `-e` option to change the range of values for the folds. This can be used to ensure some particular subset of annotations is always assigned to some fold, for example one that will always be used for train or test. Which folds are to be used for training and which for testing can be configured in the dataset configuration file. If neither `-g` nor `-l` are used, all annotations in the dataset will be split, and if the special value \"_ALL_\" for either grapheme or logogram sets is given, all sets for the chosen target will be split. In the cases before, annotations will be assigned randomly, so no proportions of graphemes/logograms/sets can be guaranteed for any fold. If the same fold proportions in each set are desired, run the command once for each of them. Options: -g, --grapheme-set TEXT Grapheme set(s) to split. -l, --logogram-set TEXT Logogram set(s) to split. -s, --start-fold INTEGER Minimum number to use for the folds. -e, --end-fold INTEGER Maximum number to use for the folds. --seed INTEGER A seed for the random split algorithm. --help Show this message and exit. extract \u00b6 Usage: quevedo extract [OPTIONS] Extract graphemes from annotated logograms. This command takes all the logograms in the given subset, extracts the graphemes annotated in each of them, and stores them as independent annotations (carrying over the relevant information) in the chosen grapheme subset. Options: -f, --from TEXT Logogram subset from which to extract graphemes. -t, --to TEXT Grapheme subset where to place extracted graphemes. -m, --merge Merge new graphemes with existing ones, if any. -r, --replace Replace old graphemes with new ones, if any. --help Show this message and exit. generate \u00b6 Usage: quevedo generate [OPTIONS] Generate artificial logograms from existing graphemes. This command creates new logograms in the chosen subset by randomly combining graphemes together. The generation process can be somewhat controlled in the configuration file. Since the goal of this process is to perform data augmentation for training, only graphemes in \"train\" folds will be used. Generated logograms will have the first fold use for training set as their fold. Options: -f, --from TEXT Grapheme subset to use -t, --to TEXT Logogram subset where to place generated logograms. -m, --merge Merge new logograms with existing ones, if any. -r, --replace Replace old logograms with new ones, if any. --help Show this message and exit. prepare \u00b6 Usage: quevedo prepare [OPTIONS] Create the files needed for training and using this network. The training files, net configuration, and mapping from dataset tags to net classes are stored in a directory named after the chosen net (-N flag) under the `networks` path. Options: --help Show this message and exit. train \u00b6 Usage: quevedo train [OPTIONS] Train the neural network. The training configuration and files must have been created by running the command `prepare`. The weights obtained after training are stored in the network directory: `/<dataset>/networks/<network_name>/darknet_final.weights`. Options: -c, --resume / --no-resume Start training with existing weights from a previous run --help Show this message and exit. predict \u00b6 Usage: quevedo predict [OPTIONS] Get predictions for an image using a trained neural network or pipeline. Options: -i, --image PATH Image to predict [required] --help Show this message and exit. test \u00b6 Usage: quevedo test [OPTIONS] Compute evaluation metrics for a trained neural network or pipeline. By default annotations in test folds (see train/test split) are used. Accuracy is computed, and also separate accuracies for detection and classification. The full predictions can be printed into a csv for further analysis with statistics software. Options: -p, --print / --no-print Show results in the command line --results-json / --no-results-json Print results into a `results.json` file in the network directory --predictions-csv / --no-predictions-csv Print all predictions into a `predictions.csv` file in the network directory --on-train Test the network on the train set instead of the test one --help Show this message and exit. web \u00b6 Usage: quevedo web [OPTIONS] Run a web interface to the dataset. The web application launched can be used to browse and manage the dataset files. Annotation pages are provided for both graphemes and logograms to allow visual annotation of objects. Very basic user management is also provided. Configuration can be written under the `web` key of the dataset configuration. Options: -h, --host TEXT -p, --port TEXT -m, --mount-path TEXT Mount path for the web application --browser / --no-browser Launch browser with the web app -l, --language [en|es] Language for the UI (default from config file) --help Show this message and exit. run_script \u00b6 Usage: quevedo run_script [OPTIONS] [EXTRA_ARGS]... Run a data processing script on dataset objects. The script should be in the 'scripts' directory of the dataset, and have a \"process\" method which will be called by Quevedo on each grapheme or logogram in the selected subsets. If it has an \"init\" method, it will be called once and firstmost with the dataset and any extra arguments that have been passed to this command. Options: -s, --scriptname TEXT Name of the script to run, without path or extension [required] -g, --grapheme-set TEXT Process graphemes from these sets -l, --logogram-set TEXT Process logograms from these sets --help Show this message and exit. migrate \u00b6 Usage: quevedo migrate [OPTIONS] Upgrades a dataset config and data to the latest version. DANGER! This will change your annotations. Please have a backup of your data in case something goes wrong. Options: --help Show this message and exit.","title":"Command Line Interface"},{"location":"cli/#command-line-interface","text":"Usage: quevedo [OPTIONS] COMMAND1 [ARGS]... [COMMAND2 [ARGS]...]... Quevedo is a tool for managing datasets of images with compositional semantics. This includes file management, annotation of data, and neural network training and use. The -D, -N and -P flags are global, and affect all commands used afterwards. -N and -P are exclusive. For example, to run a full experiment for neural network 'one', run: quevedo -D path/to/dataset -N one split -p 80 prepare train test Options: -D, --dataset PATH Path to the dataset to use, by default use current directory. -N, --network TEXT Neural network configuration to use. -P, --pipeline TEXT Pipeline configuration to use. --version Show the version and exit. --help Show this message and exit. Commands: add_images Import images from external directories into the dataset. config Edit dataset configuration. create Create and initialize a Quevedo dataset. extract Extract graphemes from annotated logograms. generate Generate artificial logograms from existing graphemes. info Get general status information about a dataset. migrate Upgrades a dataset config and data to the latest version. predict Get predictions for an image using a trained neural network... prepare Create the files needed for training and using this network. run_script Run a data processing script on dataset objects. split Assign annotations randomly to different folds. test Compute evaluation metrics for a trained neural network or... train Train the neural network. web Run a web interface to the dataset.","title":"Command Line Interface"},{"location":"cli/#config","text":"Usage: quevedo config [OPTIONS] Edit dataset configuration. This command is a simple convenience to launch an editor open at the configuration file (config.toml). Options: -e, --editor TEXT Editor to use instead of the automatically detected one --help Show this message and exit.","title":"config"},{"location":"cli/#info","text":"Usage: quevedo info [OPTIONS] Get general status information about a dataset. Options: --help Show this message and exit.","title":"info"},{"location":"cli/#create","text":"Usage: quevedo create [OPTIONS] Create and initialize a Quevedo dataset. Options: --help Show this message and exit.","title":"create"},{"location":"cli/#add_images","text":"Usage: quevedo add_images [OPTIONS] Import images from external directories into the dataset. For now, images need to be in the PNG format, and have 3 channels (color) and 8 bit depth. Options: -i, --image_dir PATH Directory from which to import images. [required] -g, --grapheme-set TEXT Import the images to this grapheme set. -l, --logogram-set TEXT Import the images to this logogram set. -m, --merge Merge new images with existing ones, if any. -r, --replace Replace old images with new ones, if any. --sort-numeric Sort images ids by filename (numeric). --sort-alphabetic Sort images ids by filename (alphabetic). --no-sort Don't sort images to import. [default] --help Show this message and exit.","title":"add_images"},{"location":"cli/#split","text":"Usage: quevedo split [OPTIONS] Assign annotations randomly to different folds. By default, the annotations will be split into a number of folds configured in the dataset configuration file, starting from 0. To override this, use the `-s` and `-e` option to change the range of values for the folds. This can be used to ensure some particular subset of annotations is always assigned to some fold, for example one that will always be used for train or test. Which folds are to be used for training and which for testing can be configured in the dataset configuration file. If neither `-g` nor `-l` are used, all annotations in the dataset will be split, and if the special value \"_ALL_\" for either grapheme or logogram sets is given, all sets for the chosen target will be split. In the cases before, annotations will be assigned randomly, so no proportions of graphemes/logograms/sets can be guaranteed for any fold. If the same fold proportions in each set are desired, run the command once for each of them. Options: -g, --grapheme-set TEXT Grapheme set(s) to split. -l, --logogram-set TEXT Logogram set(s) to split. -s, --start-fold INTEGER Minimum number to use for the folds. -e, --end-fold INTEGER Maximum number to use for the folds. --seed INTEGER A seed for the random split algorithm. --help Show this message and exit.","title":"split"},{"location":"cli/#extract","text":"Usage: quevedo extract [OPTIONS] Extract graphemes from annotated logograms. This command takes all the logograms in the given subset, extracts the graphemes annotated in each of them, and stores them as independent annotations (carrying over the relevant information) in the chosen grapheme subset. Options: -f, --from TEXT Logogram subset from which to extract graphemes. -t, --to TEXT Grapheme subset where to place extracted graphemes. -m, --merge Merge new graphemes with existing ones, if any. -r, --replace Replace old graphemes with new ones, if any. --help Show this message and exit.","title":"extract"},{"location":"cli/#generate","text":"Usage: quevedo generate [OPTIONS] Generate artificial logograms from existing graphemes. This command creates new logograms in the chosen subset by randomly combining graphemes together. The generation process can be somewhat controlled in the configuration file. Since the goal of this process is to perform data augmentation for training, only graphemes in \"train\" folds will be used. Generated logograms will have the first fold use for training set as their fold. Options: -f, --from TEXT Grapheme subset to use -t, --to TEXT Logogram subset where to place generated logograms. -m, --merge Merge new logograms with existing ones, if any. -r, --replace Replace old logograms with new ones, if any. --help Show this message and exit.","title":"generate"},{"location":"cli/#prepare","text":"Usage: quevedo prepare [OPTIONS] Create the files needed for training and using this network. The training files, net configuration, and mapping from dataset tags to net classes are stored in a directory named after the chosen net (-N flag) under the `networks` path. Options: --help Show this message and exit.","title":"prepare"},{"location":"cli/#train","text":"Usage: quevedo train [OPTIONS] Train the neural network. The training configuration and files must have been created by running the command `prepare`. The weights obtained after training are stored in the network directory: `/<dataset>/networks/<network_name>/darknet_final.weights`. Options: -c, --resume / --no-resume Start training with existing weights from a previous run --help Show this message and exit.","title":"train"},{"location":"cli/#predict","text":"Usage: quevedo predict [OPTIONS] Get predictions for an image using a trained neural network or pipeline. Options: -i, --image PATH Image to predict [required] --help Show this message and exit.","title":"predict"},{"location":"cli/#test","text":"Usage: quevedo test [OPTIONS] Compute evaluation metrics for a trained neural network or pipeline. By default annotations in test folds (see train/test split) are used. Accuracy is computed, and also separate accuracies for detection and classification. The full predictions can be printed into a csv for further analysis with statistics software. Options: -p, --print / --no-print Show results in the command line --results-json / --no-results-json Print results into a `results.json` file in the network directory --predictions-csv / --no-predictions-csv Print all predictions into a `predictions.csv` file in the network directory --on-train Test the network on the train set instead of the test one --help Show this message and exit.","title":"test"},{"location":"cli/#web","text":"Usage: quevedo web [OPTIONS] Run a web interface to the dataset. The web application launched can be used to browse and manage the dataset files. Annotation pages are provided for both graphemes and logograms to allow visual annotation of objects. Very basic user management is also provided. Configuration can be written under the `web` key of the dataset configuration. Options: -h, --host TEXT -p, --port TEXT -m, --mount-path TEXT Mount path for the web application --browser / --no-browser Launch browser with the web app -l, --language [en|es] Language for the UI (default from config file) --help Show this message and exit.","title":"web"},{"location":"cli/#run_script","text":"Usage: quevedo run_script [OPTIONS] [EXTRA_ARGS]... Run a data processing script on dataset objects. The script should be in the 'scripts' directory of the dataset, and have a \"process\" method which will be called by Quevedo on each grapheme or logogram in the selected subsets. If it has an \"init\" method, it will be called once and firstmost with the dataset and any extra arguments that have been passed to this command. Options: -s, --scriptname TEXT Name of the script to run, without path or extension [required] -g, --grapheme-set TEXT Process graphemes from these sets -l, --logogram-set TEXT Process logograms from these sets --help Show this message and exit.","title":"run_script"},{"location":"cli/#migrate","text":"Usage: quevedo migrate [OPTIONS] Upgrades a dataset config and data to the latest version. DANGER! This will change your annotations. Please have a backup of your data in case something goes wrong. Options: --help Show this message and exit.","title":"migrate"},{"location":"concepts/","text":"Quevedo Datasets \u00b6 Note If you have cloned Quevedo, you can find an example dataset in the folder examples/toy_arithmetic . Playing with this dataset might make the explanations below more understandable. Quevedo datasets consist of source images, annotations on those images, and other metadata that can help with their interpretation. While it can be used for less complex images, Quevedo's focus is on images with compositional meaning , such as constitute visual languages, like UML , or complex writing systems, like SignWriting or musical notation . Example of an UML communication diagram (source: Oemmler @ Wikipedia ) Example of a SignWriting transcription representing the ASL sign for \"SignWriting\" (source: Slevinski @ Wikipedia ) An example of modern musical notation: Prelude, Op. 28, No. 7, by Fr\u00e9d\u00e9ric Chopin (source: Prof.rick @ Wikipedia ) Logograms and Graphemes \u00b6 Quevedo recognises two types of source images: logograms and graphemes . Graphemes are atomic, individual symbols that represent some particular meaning in the target visual language, while logograms are images made up of graphemes in complex and meaningful spatial arrangements. In the UML example above, the different boxes, arrows and characters are graphemes. In the SignWriting example, the hand symbols along with the arrows indicating movement are the graphemes. In the sheet music excerpt, one can identify the notes, accidentals and other symbols as graphemes. As for logograms, what constitutes a logogram depends on the target language and the goal of the researcher, but to Quevedo, any logogram is an image file where graphemes are arranged according to some underlying meaning. The names logogram and grapheme come from the original problem for which Quevedo has been designed, which is automatic recognition of visual languages, but the software imposes little meaning to the terms beyond the fact that graphemes are independent and atomic, and logograms are composed of spatially arranged graphemes. Therefore, Quevedo can be used to manage datasets for problems of varying complexity, as long as the source data are images with some compositional structure. Annotation of logograms and graphemes \u00b6 One of the characteristics of visual writing systems is that they can encode multiple meanings within a single symbol, taking advantage of the possibilities offered by the visual medium. In Quevedo, annotation consist not of a single tag, but rather of a dictionary of tag names and values. This allows different systems to use different aspects of the symbols' meaning, and also lets researchers experiment with different, simultaneous and possibly overlapping annotation schemas for the dataset. Each annotation, both logograms and graphemes, has one such dictionary of tags associated, manually entered by an annotator or automatically filled by some process. Logograms, additionally, have a list of graphemes found within them, and each of these also has a dictionary of tags. Independent annotations (so, not the graphemes within logograms) can also have \"meta\" tags which can be used to represent other information, such as source of the annotation, status, notes, etc. To represent the spatial relations between graphemes, logograms also contain a graph of edges between the graphemes. These edges again have their own annotation schema and dictionary of tags. New in v.3 Logogram graphs are new in version 1.3. Example of the annotation of a logogram in the web interface Dataset structure \u00b6 Each Quevedo dataset is a directory on disk, containing a configuration file named config.toml , and a number of directories. Files which Quevedo doesn't recognize will be ignored, so it is safe to add these files that other programs (such as git or DVC ) may need. Annotations are stored in subdirectories of the logograms and graphemes directories (depending on their type). Each subdirectory represents a data subset, which can be used to perform different experiments on different sets, or just to organize data in some meaningful way. Annotations in each subset consist of two files: <number>.png and <number>.json . The .png file is the source image, in PNG format, and the .json file contains the annotations in JSON format. These are standard formats, so annotations in a Quevedo dataset can be read and modified by external tools and inspected by humans. The annotations are sequentially numbered, so corresponding images and json files are easily found. There are two additional directories which Quevedo uses: networks and scripts . In the networks directory, the training configuration and weight files for each different neural network are stored. Each network has a name, and its files are all organized in the subdirectory of networks with the network name. The scripts directory can contain useful scripts for additional management of the dataset. For example, a researcher can store the .r scripts used to evaluate different metrics on the dataset, or shell scripts to process images or extract annotation information. A special case are python files (ending in .py ) which Quevedo can understand . dataset_root \u251c\u2500 config.toml \u251c\u2500 logograms \u2502 \u251c\u2500 subset_1 \u2502 \u2502 \u251c\u2500 1.png \u2502 \u2502 \u251c\u2500 1.json \u2502 \u2502 \u251c\u2500 2.png \u2502 \u2502 \u251c\u2500 2.json \u2502 \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 other_subset \u251c\u2500 graphemes \u2502 \u251c\u2500 subset_1 \u2502 \u2502 \u251c\u2500 1.png \u2502 \u2502 \u251c\u2500 1.json \u2502 \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 other_subset \u251c\u2500 networks \u2502 \u251c\u2500 network_1 \u2502 \u2502 \u251c\u2500 train \u2502 \u2502 \u251c\u2500 darknet.cfg \u2502 \u2502 \u251c\u2500 darknet_final.weights \u2502 \u2502 \u251c\u2500 results.json \u2502 \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 network_2 \u2514\u2500 scripts Example of a Quevedo dataset directory structure Interaction with git and DVC \u00b6 Since Quevedo datasets are directories on disk, and the different files use standard formats, Quevedo datasets can interact nicely with other tools, such as git and DVC . In particular, a Quevedo dataset can also be a git repository, and therefore a DVC repository too. This can help with dataset sharing and experiment reproducibility. We recommend using git to track configuration files and scripts, and DVC to track source data, annotations, and experiments.","title":"Quevedo Datasets"},{"location":"concepts/#quevedo-datasets","text":"Note If you have cloned Quevedo, you can find an example dataset in the folder examples/toy_arithmetic . Playing with this dataset might make the explanations below more understandable. Quevedo datasets consist of source images, annotations on those images, and other metadata that can help with their interpretation. While it can be used for less complex images, Quevedo's focus is on images with compositional meaning , such as constitute visual languages, like UML , or complex writing systems, like SignWriting or musical notation . Example of an UML communication diagram (source: Oemmler @ Wikipedia ) Example of a SignWriting transcription representing the ASL sign for \"SignWriting\" (source: Slevinski @ Wikipedia ) An example of modern musical notation: Prelude, Op. 28, No. 7, by Fr\u00e9d\u00e9ric Chopin (source: Prof.rick @ Wikipedia )","title":"Quevedo Datasets"},{"location":"concepts/#logograms-and-graphemes","text":"Quevedo recognises two types of source images: logograms and graphemes . Graphemes are atomic, individual symbols that represent some particular meaning in the target visual language, while logograms are images made up of graphemes in complex and meaningful spatial arrangements. In the UML example above, the different boxes, arrows and characters are graphemes. In the SignWriting example, the hand symbols along with the arrows indicating movement are the graphemes. In the sheet music excerpt, one can identify the notes, accidentals and other symbols as graphemes. As for logograms, what constitutes a logogram depends on the target language and the goal of the researcher, but to Quevedo, any logogram is an image file where graphemes are arranged according to some underlying meaning. The names logogram and grapheme come from the original problem for which Quevedo has been designed, which is automatic recognition of visual languages, but the software imposes little meaning to the terms beyond the fact that graphemes are independent and atomic, and logograms are composed of spatially arranged graphemes. Therefore, Quevedo can be used to manage datasets for problems of varying complexity, as long as the source data are images with some compositional structure.","title":"Logograms and Graphemes"},{"location":"concepts/#annotation-of-logograms-and-graphemes","text":"One of the characteristics of visual writing systems is that they can encode multiple meanings within a single symbol, taking advantage of the possibilities offered by the visual medium. In Quevedo, annotation consist not of a single tag, but rather of a dictionary of tag names and values. This allows different systems to use different aspects of the symbols' meaning, and also lets researchers experiment with different, simultaneous and possibly overlapping annotation schemas for the dataset. Each annotation, both logograms and graphemes, has one such dictionary of tags associated, manually entered by an annotator or automatically filled by some process. Logograms, additionally, have a list of graphemes found within them, and each of these also has a dictionary of tags. Independent annotations (so, not the graphemes within logograms) can also have \"meta\" tags which can be used to represent other information, such as source of the annotation, status, notes, etc. To represent the spatial relations between graphemes, logograms also contain a graph of edges between the graphemes. These edges again have their own annotation schema and dictionary of tags. New in v.3 Logogram graphs are new in version 1.3. Example of the annotation of a logogram in the web interface","title":"Annotation of logograms and graphemes"},{"location":"concepts/#dataset-structure","text":"Each Quevedo dataset is a directory on disk, containing a configuration file named config.toml , and a number of directories. Files which Quevedo doesn't recognize will be ignored, so it is safe to add these files that other programs (such as git or DVC ) may need. Annotations are stored in subdirectories of the logograms and graphemes directories (depending on their type). Each subdirectory represents a data subset, which can be used to perform different experiments on different sets, or just to organize data in some meaningful way. Annotations in each subset consist of two files: <number>.png and <number>.json . The .png file is the source image, in PNG format, and the .json file contains the annotations in JSON format. These are standard formats, so annotations in a Quevedo dataset can be read and modified by external tools and inspected by humans. The annotations are sequentially numbered, so corresponding images and json files are easily found. There are two additional directories which Quevedo uses: networks and scripts . In the networks directory, the training configuration and weight files for each different neural network are stored. Each network has a name, and its files are all organized in the subdirectory of networks with the network name. The scripts directory can contain useful scripts for additional management of the dataset. For example, a researcher can store the .r scripts used to evaluate different metrics on the dataset, or shell scripts to process images or extract annotation information. A special case are python files (ending in .py ) which Quevedo can understand . dataset_root \u251c\u2500 config.toml \u251c\u2500 logograms \u2502 \u251c\u2500 subset_1 \u2502 \u2502 \u251c\u2500 1.png \u2502 \u2502 \u251c\u2500 1.json \u2502 \u2502 \u251c\u2500 2.png \u2502 \u2502 \u251c\u2500 2.json \u2502 \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 other_subset \u251c\u2500 graphemes \u2502 \u251c\u2500 subset_1 \u2502 \u2502 \u251c\u2500 1.png \u2502 \u2502 \u251c\u2500 1.json \u2502 \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 other_subset \u251c\u2500 networks \u2502 \u251c\u2500 network_1 \u2502 \u2502 \u251c\u2500 train \u2502 \u2502 \u251c\u2500 darknet.cfg \u2502 \u2502 \u251c\u2500 darknet_final.weights \u2502 \u2502 \u251c\u2500 results.json \u2502 \u2502 \u2514\u2500 ... \u2502 \u2514\u2500 network_2 \u2514\u2500 scripts Example of a Quevedo dataset directory structure","title":"Dataset structure"},{"location":"concepts/#interaction-with-git-and-dvc","text":"Since Quevedo datasets are directories on disk, and the different files use standard formats, Quevedo datasets can interact nicely with other tools, such as git and DVC . In particular, a Quevedo dataset can also be a git repository, and therefore a DVC repository too. This can help with dataset sharing and experiment reproducibility. We recommend using git to track configuration files and scripts, and DVC to track source data, annotations, and experiments.","title":"Interaction with git and DVC"},{"location":"config/","text":"Dataset Configuration \u00b6 All configuration of a Quevedo dataset is found in its configuration file, which is found at the root of the dataset and named config.toml . This file is in TOML format, which makes it ideal for both human and machine editing. We recommend reading TOML documentation to really understand the format, but it is an intuitive enough language that you can understand the configuration file enough to modify it just by reading it. As a convenience, quevedo provides the quevedo config command to edit the configuration file, but this only launches the user's configured text editor with the config.toml file open. Local configuration \u00b6 Quevedo datasets are meant to be shared, and configuration is an essential part of the dataset. However, some options may be applicable only for the local environment, and others may be sensitive and best not distributed. For this, Quevedo also reads a config.local.toml if present. The options in the local configuration file are merged with those in the main file, overriding them when there is a conflict. This can be useful for the configuration of darknet installation, which is likely different for different environments, and for the web interface, which may contain sensitive information like secrets and users' passwords (even if hashed). Annotation schema \u00b6 The annotation schema of a dataset is a complex set of information and decisions, but to Quevedo, the important information is the featuers that graphemes, logograms and edges can have. These are lists of strings, and each string represents a possible feature for an annotation object. Each concrete object, then, has a particular value (another string) for each of the appropriate features in its schema. There are four schemas: g_tags : Possible features for each grapheme, either bound or isolated. l_tags : Features for each of the logograms. e_tags : Features for the edges of the logogram graph. meta_tags : Additional information that can be stored for isolated annotation files, either graphemes or logograms, but not for bound graphemes. Tags are represented as dictionary objects both in the annotation files (in json format) and in the code (python dict s). Warning In versions of Quevedo before v1.1, tags were stored as a list instead of a dictionary. Before v1.3, there was no logogram annotation schema. If your dataset is using an old structure, Quevedo will warn you. Please run the migrate command to upgrade the dataset. In the annotation file, apart from their own tags , logograms have a list of graphemes found within them. These bound graphemes have their own tags from the g_tags schema, and an additional piece of information: the coordinates where they can be found within the logogram image (a.k.a bounding boxes). Other options \u00b6 darknet : Configuration for using the darknet binary and library. See Darknet installation . network : Configuration for training and using neural networks. See Network configuration . pipeline : Configuration for pipelines which use many networks to solve a task. See Pipeline configuration . web : Configuration for the web interface. See Web interface configuration . generate : These options guide the process of artificial logogram generation used for data augmentation. See generate . folds , train_folds , test_folds : The folds option sets the default folds that the split will use to partition annotations. The train_folds option is a list of fold values that will be used to train, and the test_folds option respectively for testing. See Splits and folds for more. Default configuration \u00b6 When creating a dataset, Quevedo places a default configuration file with comments to ease personalization. The default file is included here for reference: # This file is a Quevedo dataset configuration file. Find more at: # https://www.github.com/agarsev/quevedo # Local overrides can be written in `config.local.toml` title = \"The title\" description = \"\"\" The dataset description. \"\"\" tag_schema = [ \"tag\" ] # For a multi-tag schema, use: # tag_schema = [ \"tag1\", \"tag2\" ] # Meta tags affect the whole annotation, rather than individual graphemes. The # first one will be used as title for the annotation in the web listing. meta_tags = [ \"filename\", \"meaning\" ] # Flags are also meta tags, but can only be true/false. They are displayed in # the web interface as checkboxes with the icon set here. flags = { done = \"\u2714\ufe0f\", problem = \"\u26a0\ufe0f\", notes = \"\ud83d\udcdd\" } annotation_help = \"\"\" Write here any help for annotators, like lists of graphemes or other instructions. For example: Make boxes slightly larger than the graphemes, not too tight. \"\"\" config_version = 1 # Version of quevedo dataset schema, not of dataset data # Number of folds to split annotations into, and which to use for training and # which to use for testing folds = 10 train_folds = [0,1,2,3,4,5,6,7] test_folds = [8,9] [darknet] path = \"darknet/darknet\" library = \"darknet/libdarknet.so\" options = [ \"-dont_show\", \"-mjpeg_port\", \"8090\" ] [network.one] default = true # Network to use if not specified task = \"detect\" # tag = \"tag\" # Uncomment and choose a tag name from tag_schema to use # subsets = [ \"default\" ] # If not specified, all subsets will be used subject = \"Focus on grapheme type learning and recognition\" [network.two] task = \"classify\" # tag = \"tag\" # tag = [ \"tag1\", \"tag2\" ] # A combination of tags can be used as the network \"class\" subsets = [ \"default\" ] subject = \"Classify graphemes\" # A filter can be used to select the annotations to use for training this network # [network.two.filter] # criterion = \"tag\" # Tag to use to decide whether to include or not each annotation # include = [ \"value\" ] # Values for the criterion to use # # exclude = [ \"value\" ] # Alternatively, values to exclude # Automatic data augmentation can be configured here: # [network.two.augment] # angle = 10 # flip = 1 # yes/no, 1/0 # exposure = 0.8 # aspect = 0.8 # only for classify tasks # Add more networks like so: # [network.other] # task = \"detect\" or \"classify\" # tag = \"tag\" # subject = \"human readable description\" # ... [web] host = \"localhost\" port = \"5000\" mount_path = \"\" lang = \"en\" public = true # Set to false to require login # Generate your own secret key with, for example: # python -c 'from secrets import token_hex; print(token_hex(16))' secret_key = \"ce8c9cd0316faac773645648ac827ff6\" # [web.users.annotator] # # Uncomment and modify to add users # # Hash the password with: # # python -c 'import hashlib; print(hashlib.new(\"sha1\", \"thepassword\".encode(\"utf8\")).hexdigest());' # password = \"\" # read = \"ALL\" # Can read all subsets # # read = [ \"public\" ] # Can read all subsets that contain the string 'public' # write = \"NONE\" # Can't write (modify) any subsets # # write = [ \"set1$$\", \"set2$$\" ] # Can write to set1 and set2, both logogram or # # graphemes (they are regexes) # # [web.users.user2] # ... [generate] # Configuration for the artificial logogram generation count = 500 width_range = [ 200, 300 ] height_range = [ 200, 300 ] tag = \"tag\" # Tag to guide grapheme placement [[generate.params]] match = 'one' # Match graphemes tagged with class \"one\" mode = 'one' # Only put one of these graphemes freq = 0.4 # How often to add one of these rotate = false [[generate.params]] match = 'excluded' mode = 'none' # Don't put any of these graphemes [[generate.params]] match = '.*' # Match any grapheme (it's a regex) mode = 'many' # Add potentially many of these graphemes max = 3 # How many to potentially add prob = 0.6 # Probability for a single grapheme to appear (times max = expected number) rotate = true","title":"Dataset Configuration"},{"location":"config/#dataset-configuration","text":"All configuration of a Quevedo dataset is found in its configuration file, which is found at the root of the dataset and named config.toml . This file is in TOML format, which makes it ideal for both human and machine editing. We recommend reading TOML documentation to really understand the format, but it is an intuitive enough language that you can understand the configuration file enough to modify it just by reading it. As a convenience, quevedo provides the quevedo config command to edit the configuration file, but this only launches the user's configured text editor with the config.toml file open.","title":"Dataset Configuration"},{"location":"config/#local-configuration","text":"Quevedo datasets are meant to be shared, and configuration is an essential part of the dataset. However, some options may be applicable only for the local environment, and others may be sensitive and best not distributed. For this, Quevedo also reads a config.local.toml if present. The options in the local configuration file are merged with those in the main file, overriding them when there is a conflict. This can be useful for the configuration of darknet installation, which is likely different for different environments, and for the web interface, which may contain sensitive information like secrets and users' passwords (even if hashed).","title":"Local configuration"},{"location":"config/#annotation-schema","text":"The annotation schema of a dataset is a complex set of information and decisions, but to Quevedo, the important information is the featuers that graphemes, logograms and edges can have. These are lists of strings, and each string represents a possible feature for an annotation object. Each concrete object, then, has a particular value (another string) for each of the appropriate features in its schema. There are four schemas: g_tags : Possible features for each grapheme, either bound or isolated. l_tags : Features for each of the logograms. e_tags : Features for the edges of the logogram graph. meta_tags : Additional information that can be stored for isolated annotation files, either graphemes or logograms, but not for bound graphemes. Tags are represented as dictionary objects both in the annotation files (in json format) and in the code (python dict s). Warning In versions of Quevedo before v1.1, tags were stored as a list instead of a dictionary. Before v1.3, there was no logogram annotation schema. If your dataset is using an old structure, Quevedo will warn you. Please run the migrate command to upgrade the dataset. In the annotation file, apart from their own tags , logograms have a list of graphemes found within them. These bound graphemes have their own tags from the g_tags schema, and an additional piece of information: the coordinates where they can be found within the logogram image (a.k.a bounding boxes).","title":"Annotation schema"},{"location":"config/#other-options","text":"darknet : Configuration for using the darknet binary and library. See Darknet installation . network : Configuration for training and using neural networks. See Network configuration . pipeline : Configuration for pipelines which use many networks to solve a task. See Pipeline configuration . web : Configuration for the web interface. See Web interface configuration . generate : These options guide the process of artificial logogram generation used for data augmentation. See generate . folds , train_folds , test_folds : The folds option sets the default folds that the split will use to partition annotations. The train_folds option is a list of fold values that will be used to train, and the test_folds option respectively for testing. See Splits and folds for more.","title":"Other options"},{"location":"config/#default-configuration","text":"When creating a dataset, Quevedo places a default configuration file with comments to ease personalization. The default file is included here for reference: # This file is a Quevedo dataset configuration file. Find more at: # https://www.github.com/agarsev/quevedo # Local overrides can be written in `config.local.toml` title = \"The title\" description = \"\"\" The dataset description. \"\"\" tag_schema = [ \"tag\" ] # For a multi-tag schema, use: # tag_schema = [ \"tag1\", \"tag2\" ] # Meta tags affect the whole annotation, rather than individual graphemes. The # first one will be used as title for the annotation in the web listing. meta_tags = [ \"filename\", \"meaning\" ] # Flags are also meta tags, but can only be true/false. They are displayed in # the web interface as checkboxes with the icon set here. flags = { done = \"\u2714\ufe0f\", problem = \"\u26a0\ufe0f\", notes = \"\ud83d\udcdd\" } annotation_help = \"\"\" Write here any help for annotators, like lists of graphemes or other instructions. For example: Make boxes slightly larger than the graphemes, not too tight. \"\"\" config_version = 1 # Version of quevedo dataset schema, not of dataset data # Number of folds to split annotations into, and which to use for training and # which to use for testing folds = 10 train_folds = [0,1,2,3,4,5,6,7] test_folds = [8,9] [darknet] path = \"darknet/darknet\" library = \"darknet/libdarknet.so\" options = [ \"-dont_show\", \"-mjpeg_port\", \"8090\" ] [network.one] default = true # Network to use if not specified task = \"detect\" # tag = \"tag\" # Uncomment and choose a tag name from tag_schema to use # subsets = [ \"default\" ] # If not specified, all subsets will be used subject = \"Focus on grapheme type learning and recognition\" [network.two] task = \"classify\" # tag = \"tag\" # tag = [ \"tag1\", \"tag2\" ] # A combination of tags can be used as the network \"class\" subsets = [ \"default\" ] subject = \"Classify graphemes\" # A filter can be used to select the annotations to use for training this network # [network.two.filter] # criterion = \"tag\" # Tag to use to decide whether to include or not each annotation # include = [ \"value\" ] # Values for the criterion to use # # exclude = [ \"value\" ] # Alternatively, values to exclude # Automatic data augmentation can be configured here: # [network.two.augment] # angle = 10 # flip = 1 # yes/no, 1/0 # exposure = 0.8 # aspect = 0.8 # only for classify tasks # Add more networks like so: # [network.other] # task = \"detect\" or \"classify\" # tag = \"tag\" # subject = \"human readable description\" # ... [web] host = \"localhost\" port = \"5000\" mount_path = \"\" lang = \"en\" public = true # Set to false to require login # Generate your own secret key with, for example: # python -c 'from secrets import token_hex; print(token_hex(16))' secret_key = \"ce8c9cd0316faac773645648ac827ff6\" # [web.users.annotator] # # Uncomment and modify to add users # # Hash the password with: # # python -c 'import hashlib; print(hashlib.new(\"sha1\", \"thepassword\".encode(\"utf8\")).hexdigest());' # password = \"\" # read = \"ALL\" # Can read all subsets # # read = [ \"public\" ] # Can read all subsets that contain the string 'public' # write = \"NONE\" # Can't write (modify) any subsets # # write = [ \"set1$$\", \"set2$$\" ] # Can write to set1 and set2, both logogram or # # graphemes (they are regexes) # # [web.users.user2] # ... [generate] # Configuration for the artificial logogram generation count = 500 width_range = [ 200, 300 ] height_range = [ 200, 300 ] tag = \"tag\" # Tag to guide grapheme placement [[generate.params]] match = 'one' # Match graphemes tagged with class \"one\" mode = 'one' # Only put one of these graphemes freq = 0.4 # How often to add one of these rotate = false [[generate.params]] match = 'excluded' mode = 'none' # Don't put any of these graphemes [[generate.params]] match = '.*' # Match any grapheme (it's a regex) mode = 'many' # Add potentially many of these graphemes max = 3 # How many to potentially add prob = 0.6 # Probability for a single grapheme to appear (times max = expected number) rotate = true","title":"Default configuration"},{"location":"dev/","text":"Quevedo as a library \u00b6 Quevedo can be used as a command line application to manage a dataset, but it can also be used from other Python code to make programatic access to the dataset more convenient, or in user scripts run by Quevedo on the dataset objects. Call from python \u00b6 To use Quevedo from other python code, you can import it with import quevedo , or it may be more useful to directly import the Dataset class: from quevedo import Dataset . This class has most of the functionality to deal with Quevedo datasets, including managing the data and the neural networks. There are some examples of use in the examples directory of the repo, and we try to keep the code readable to help understand the library. The full API reference with the different classess and methods can be read here . User scripts \u00b6 Every dataset is different, and dealing with data often needs to have custom procedures and algorithms, specific to the problem at hand. We suggest to place these scripts in the scripts directory of the dataset, to keep them organized. Quevedo can also help run scripts in this directory using the command run_script . With run_script , you don't need to write the boilerplate code of accessing all the annotations, loading their data and image, and saving them. Just provide a process function, which receives an annotation object and the dataset, and process the annotation with your custom logic. The run_script command then lets you specify, using syntax similar to the other commands, what subsets to run the script in. For example: from datetime import date from quevedo import Annotation, Dataset # Our custom logic to get tags from the filename def tags_from_filename(filename: str): tags = filename.split('_') if tags[0] == 'something': tags[0] = 'some other thing' return tags def process(a: Annotation, ds: Dataset): if a.meta['author'] is not None: return False # We don't want to modify this annotation a.meta['annotation_date'] = date.today() a.meta['author'] = 'automatic' # The original filename is kept by `add_images` a.tags = tags_from_filename(a.meta['filename']) # We have updated the annotation, so return True for Quevedo to save it return True Another advantage of user scripts is that Quevedo makes them available on the web interface . The top right corner of the annotation page has a listing of functions, including trained neural networks and user scripts, that can be run, allowing annotators to access this functionality directly from the web interface. If the script is used from the web interface, the annotation won't be automatically saved, allowing the user to review the results before clicking save. Modifying Quevedo \u00b6 Quevedo is open source! You can modify and extend it by forking it on GitHub . If you use Quevedo for your research and have ideas for improvement, please do get in touch via GitHub discussions or email.","title":"Quevedo as a library"},{"location":"dev/#quevedo-as-a-library","text":"Quevedo can be used as a command line application to manage a dataset, but it can also be used from other Python code to make programatic access to the dataset more convenient, or in user scripts run by Quevedo on the dataset objects.","title":"Quevedo as a library"},{"location":"dev/#call-from-python","text":"To use Quevedo from other python code, you can import it with import quevedo , or it may be more useful to directly import the Dataset class: from quevedo import Dataset . This class has most of the functionality to deal with Quevedo datasets, including managing the data and the neural networks. There are some examples of use in the examples directory of the repo, and we try to keep the code readable to help understand the library. The full API reference with the different classess and methods can be read here .","title":"Call from python"},{"location":"dev/#user-scripts","text":"Every dataset is different, and dealing with data often needs to have custom procedures and algorithms, specific to the problem at hand. We suggest to place these scripts in the scripts directory of the dataset, to keep them organized. Quevedo can also help run scripts in this directory using the command run_script . With run_script , you don't need to write the boilerplate code of accessing all the annotations, loading their data and image, and saving them. Just provide a process function, which receives an annotation object and the dataset, and process the annotation with your custom logic. The run_script command then lets you specify, using syntax similar to the other commands, what subsets to run the script in. For example: from datetime import date from quevedo import Annotation, Dataset # Our custom logic to get tags from the filename def tags_from_filename(filename: str): tags = filename.split('_') if tags[0] == 'something': tags[0] = 'some other thing' return tags def process(a: Annotation, ds: Dataset): if a.meta['author'] is not None: return False # We don't want to modify this annotation a.meta['annotation_date'] = date.today() a.meta['author'] = 'automatic' # The original filename is kept by `add_images` a.tags = tags_from_filename(a.meta['filename']) # We have updated the annotation, so return True for Quevedo to save it return True Another advantage of user scripts is that Quevedo makes them available on the web interface . The top right corner of the annotation page has a listing of functions, including trained neural networks and user scripts, that can be run, allowing annotators to access this functionality directly from the web interface. If the script is used from the web interface, the annotation won't be automatically saved, allowing the user to review the results before clicking save.","title":"User scripts"},{"location":"dev/#modifying-quevedo","text":"Quevedo is open source! You can modify and extend it by forking it on GitHub . If you use Quevedo for your research and have ideas for improvement, please do get in touch via GitHub discussions or email.","title":"Modifying Quevedo"},{"location":"guide/","text":"Building a Dataset \u00b6 In this guide, we give an example of the commands and steps needed to create and use a Quevedo dataset. It might be helpful to have an environment available where you can test the different commands as you read the guide, and maybe some data that you can import. The process of creating a dataset is often not straightforward or works out on the first step, so don't worry about making mistakes and having to repeat the process (but please don't delete your original data! Keep those safe in some backup or cloud. Quevedo only works with data it has copied, so deleting a Quevedo dataset is safe as long as you keep copies of your original data somewhere). In this guide we use git and DVC to manage repository versions and workflows, but that is not necessary, so you can ignore those steps if you don't use them. Also, we assume Quevedo is installed and available as the quevedo command, if not, please follow the steps here . Create repo \u00b6 To initialize the directory where the data and annotations will live, use the create command: $ quevedo -D dataset_name create It will offer you the opportunity to customize the configuration file for the repository, and set your annotation schema and other information. You can modify it later by editing the config.toml file, or using the config command. From this point on, we will run commands with the dataset directory as working dir, so change to it with cd dataset_name , and we won't need the -D flag anymore. If you want to use git and/or DVC, initialize the repository with the commands: $ git init $ git add -A . $ git commit -m \"Created quevedo repository\" $ dvc init $ git commit -m \"Initialize DVC\" Add data \u00b6 Once we have the structure, the first step is to import our data. This can be done using the add_images command, specifying both the source directory and target subset. To specify the target subset, use the -l flag if it's a logogram subset, or -g for graphemes. You can specify multiple source directories, which will be imported to the same subset. $ quevedo add_images -i source_image_directory -l logogram_set To track these data with DVC, run: $ dvc add logograms/* $ git add logograms/logogram_set.dvc logograms/.gitignore $ git commit -m \"Imported logograms\" Automatically process the data \u00b6 After the images are imported, we may want to use some preliminary automatic processing, like adding some tags that can be determined by code, preprocessing the images, etc. Create a scripts directory if it doesn't exist, and write your code there according to the user script documentation . Then you can run it on the appropriate subsets with the run_script command. $ mkdir scripts $ vim scripts/script_name.py $ quevedo run_script -s script_name -l logogram_set $ dvc add logograms Annotate the data \u00b6 Most of the important information in a dataset, apart to the source data, are the human annotations on these data (otherwise, why bother, right?). Since Quevedo deals with visual data, a graphical interface is needed for annotation, and is provided in the form of a web interface . Remember to first set in the configuration file the annotation schema that you want to use, and then you can lanch the server with the web command. If using git and dvc, remember to add and commit any modifications. $ quevedo web $ dvc add logograms $ git commit -m \"Annotated logograms\" Augment the data \u00b6 Once logograms are manually annotated, Quevedo can extract the graphemes included within them to augment the number of grapheme instances available to us, with the extract command. If what we have are graphemes, we can generate artificial examples of logograms with the generate . With these two commands, the data available for training increase, hopefully improving our algorithms. $ quevedo extract -f logogram_set -t extracted_grapheme_set $ quevedo generate -f grapheme_set -t generated_logogram_set These steps can be added to a DVC pipelines file so that DVC tracks the procedure and the results, and when we distribute the dataset other people can reproduce the full process. To have dvc automatically fill the pipelines file, run the commands with dvc run : $ dvc run -n extract \\ -d logograms/logogram_set \\ -o graphemes/extracted_set \\ quevedo extract -f logogram_set -t extracted_set Splits and folds \u00b6 New in v1.1 In previous versions of Quevedo, train and test splits worked differently. If your dataset is from one of these versions, please use the migrate command to upgrade, and do the partitioning again. For experimentation, we often need to divide our files into a train split on which to train the algorithms, and a test or evaluation split which acts as a stand-in for \"new\" or \"unknown\" data. We may also want to do cross-validation, in which evaluation is done on different runs of the experiment using different train/test partitions. Or in other cases, we may want to exclude some data from all training and testing, making a heldout set which is only looked at in the very end to really evaluate performance. To support different needs from the researchers, the strategy adopted by Quevedo is to assign annotations to \"folds\". Then, groups of folds can be defined either as being used for training, testing, or none. What folds to use, and which to assign to training or testing is set in the configuration file . Quevedo can assign the folds to your annotations randomly so that different folds have the approximate same number of annotations using the split command: Split all logograms into the default folds: $ quevedo split -l _ALL_ Assign all graphemes in \"some_set\" to either fold 0 or 1: $ quevedo split -g some_set -s 0 -e 1 Train and test the neural network \u00b6 Now that our data are properly organized and annotated, we can try training a neural network and evaluating its results. The first step is to prepare the files needed for training, then calling the darknet binary with the train command. Finally, the test command evalates some quick metrics on the trained neural networks, and can also print all predictions so you can use your statistical software to get a more in-depth analysis. The commands can also be chained, so it is enough to run (but it will probably take some time): $ quevedo prepare train test Remember that first you must have installed darknet and configured the neural network in the Quevedo configuration file. If you have more than one network, specify which one to use with the -N flag: $ quevedo -N other_network prepare train test To keep track of the neural network in DVC, we recommend setting preparation, training and test as different stages in the pipeline, so that intermediate artifacts can be cached and the expensive process of training only performed if necessary, and letting DVC track the produced metrics. If you have different networks, they can be set up as template parameters in the pipelines file to keep things DRY . $ dvc run -n prepare_detect \\ -d logograms \\ -o networks/detect/train.txt \\ quevedo -N detect prepare $ dvc run -n train_detect \\ -d networks/detect/train.txt \\ -o networks/detect/darknet_final.weights \\ quevedo -N detect train $ dvc run -n test_detect \\ -d networks/detect/darknet_final.weights \\ -m networks/detect/results.json \\ quevedo -N detect test --results-json Exploitation \u00b6 When doing data science, sometimes it is enough to stop at this step. Data are annotated, neural networks trained, experiments run and conclusions obtained. But often the results are actually useful beyond the science, and we want to somehow peruse them. The trained neural network weights are stored in the network directory , and can be used with darknet in other applications or loaded by OpenCV for example. If access to the dataset data is needed, and not only the training results, Quevedo can also be used as a library from your own code.","title":"Building a Dataset"},{"location":"guide/#building-a-dataset","text":"In this guide, we give an example of the commands and steps needed to create and use a Quevedo dataset. It might be helpful to have an environment available where you can test the different commands as you read the guide, and maybe some data that you can import. The process of creating a dataset is often not straightforward or works out on the first step, so don't worry about making mistakes and having to repeat the process (but please don't delete your original data! Keep those safe in some backup or cloud. Quevedo only works with data it has copied, so deleting a Quevedo dataset is safe as long as you keep copies of your original data somewhere). In this guide we use git and DVC to manage repository versions and workflows, but that is not necessary, so you can ignore those steps if you don't use them. Also, we assume Quevedo is installed and available as the quevedo command, if not, please follow the steps here .","title":"Building a Dataset"},{"location":"guide/#create-repo","text":"To initialize the directory where the data and annotations will live, use the create command: $ quevedo -D dataset_name create It will offer you the opportunity to customize the configuration file for the repository, and set your annotation schema and other information. You can modify it later by editing the config.toml file, or using the config command. From this point on, we will run commands with the dataset directory as working dir, so change to it with cd dataset_name , and we won't need the -D flag anymore. If you want to use git and/or DVC, initialize the repository with the commands: $ git init $ git add -A . $ git commit -m \"Created quevedo repository\" $ dvc init $ git commit -m \"Initialize DVC\"","title":"Create repo"},{"location":"guide/#add-data","text":"Once we have the structure, the first step is to import our data. This can be done using the add_images command, specifying both the source directory and target subset. To specify the target subset, use the -l flag if it's a logogram subset, or -g for graphemes. You can specify multiple source directories, which will be imported to the same subset. $ quevedo add_images -i source_image_directory -l logogram_set To track these data with DVC, run: $ dvc add logograms/* $ git add logograms/logogram_set.dvc logograms/.gitignore $ git commit -m \"Imported logograms\"","title":"Add data"},{"location":"guide/#automatically-process-the-data","text":"After the images are imported, we may want to use some preliminary automatic processing, like adding some tags that can be determined by code, preprocessing the images, etc. Create a scripts directory if it doesn't exist, and write your code there according to the user script documentation . Then you can run it on the appropriate subsets with the run_script command. $ mkdir scripts $ vim scripts/script_name.py $ quevedo run_script -s script_name -l logogram_set $ dvc add logograms","title":"Automatically process the data"},{"location":"guide/#annotate-the-data","text":"Most of the important information in a dataset, apart to the source data, are the human annotations on these data (otherwise, why bother, right?). Since Quevedo deals with visual data, a graphical interface is needed for annotation, and is provided in the form of a web interface . Remember to first set in the configuration file the annotation schema that you want to use, and then you can lanch the server with the web command. If using git and dvc, remember to add and commit any modifications. $ quevedo web $ dvc add logograms $ git commit -m \"Annotated logograms\"","title":"Annotate the data"},{"location":"guide/#augment-the-data","text":"Once logograms are manually annotated, Quevedo can extract the graphemes included within them to augment the number of grapheme instances available to us, with the extract command. If what we have are graphemes, we can generate artificial examples of logograms with the generate . With these two commands, the data available for training increase, hopefully improving our algorithms. $ quevedo extract -f logogram_set -t extracted_grapheme_set $ quevedo generate -f grapheme_set -t generated_logogram_set These steps can be added to a DVC pipelines file so that DVC tracks the procedure and the results, and when we distribute the dataset other people can reproduce the full process. To have dvc automatically fill the pipelines file, run the commands with dvc run : $ dvc run -n extract \\ -d logograms/logogram_set \\ -o graphemes/extracted_set \\ quevedo extract -f logogram_set -t extracted_set","title":"Augment the data"},{"location":"guide/#splits-and-folds","text":"New in v1.1 In previous versions of Quevedo, train and test splits worked differently. If your dataset is from one of these versions, please use the migrate command to upgrade, and do the partitioning again. For experimentation, we often need to divide our files into a train split on which to train the algorithms, and a test or evaluation split which acts as a stand-in for \"new\" or \"unknown\" data. We may also want to do cross-validation, in which evaluation is done on different runs of the experiment using different train/test partitions. Or in other cases, we may want to exclude some data from all training and testing, making a heldout set which is only looked at in the very end to really evaluate performance. To support different needs from the researchers, the strategy adopted by Quevedo is to assign annotations to \"folds\". Then, groups of folds can be defined either as being used for training, testing, or none. What folds to use, and which to assign to training or testing is set in the configuration file . Quevedo can assign the folds to your annotations randomly so that different folds have the approximate same number of annotations using the split command: Split all logograms into the default folds: $ quevedo split -l _ALL_ Assign all graphemes in \"some_set\" to either fold 0 or 1: $ quevedo split -g some_set -s 0 -e 1","title":"Splits and folds"},{"location":"guide/#train-and-test-the-neural-network","text":"Now that our data are properly organized and annotated, we can try training a neural network and evaluating its results. The first step is to prepare the files needed for training, then calling the darknet binary with the train command. Finally, the test command evalates some quick metrics on the trained neural networks, and can also print all predictions so you can use your statistical software to get a more in-depth analysis. The commands can also be chained, so it is enough to run (but it will probably take some time): $ quevedo prepare train test Remember that first you must have installed darknet and configured the neural network in the Quevedo configuration file. If you have more than one network, specify which one to use with the -N flag: $ quevedo -N other_network prepare train test To keep track of the neural network in DVC, we recommend setting preparation, training and test as different stages in the pipeline, so that intermediate artifacts can be cached and the expensive process of training only performed if necessary, and letting DVC track the produced metrics. If you have different networks, they can be set up as template parameters in the pipelines file to keep things DRY . $ dvc run -n prepare_detect \\ -d logograms \\ -o networks/detect/train.txt \\ quevedo -N detect prepare $ dvc run -n train_detect \\ -d networks/detect/train.txt \\ -o networks/detect/darknet_final.weights \\ quevedo -N detect train $ dvc run -n test_detect \\ -d networks/detect/darknet_final.weights \\ -m networks/detect/results.json \\ quevedo -N detect test --results-json","title":"Train and test the neural network"},{"location":"guide/#exploitation","text":"When doing data science, sometimes it is enough to stop at this step. Data are annotated, neural networks trained, experiments run and conclusions obtained. But often the results are actually useful beyond the science, and we want to somehow peruse them. The trained neural network weights are stored in the network directory , and can be used with darknet in other applications or loaded by OpenCV for example. If access to the dataset data is needed, and not only the training results, Quevedo can also be used as a library from your own code.","title":"Exploitation"},{"location":"nets/","text":"Neural networks \u00b6 One of the difficulties of processing visual languages automatically is input, when it is presented in the form of images. Images are represented digitally as collections of pixels, arrayed in memory in a way that makes sense for display and storage, but which is completely disconnected to the meaning these images have to humans. Additionally, if input is hand written, graphemes can present variations which don't affect human understanding but which mean completely different pixel patterns are present. And positioning of objects is again not based on hard rules, but rather on visual interpretation. For these reasons, machine learning techniques developed in the field of computer vision are necessary to adequately process logograms and graphemes. While the researcher can use any toolkit and algorithm they prefer, Quevedo includes a module to facilitate using neural networks with Quevedo datasets. Darknet \u00b6 Darknet is \"an open source neural network framework written in C and CUDA\", developed by the inventor of the YOLO algorithm, Joseph Redmon. This framework includes a binary and linked library which make configuring, training, and using neural networks for computer vision straightforward and efficient. The neural network module included with Quevedo needs darknet to be available. This module automatically prepares network configuration and training files from the metadata in the dataset, and can manage the training and prediction process. Installation \u00b6 We recommend using this fork by Alexey Bochkovskiy . Installation can vary depending on your environment, including the CUDA and OpenCV (optional) libraries installed, but with luck, the following will work: $ git clone https://github.com/AlexeyAB/darknet $ cd darknet <edit the Makefile> $ make In the Makefile, you probably want to enable GPU=1 and CUDNN=1 , otherwise training will be too slow. Depending on the GPU available and CUDA installation, you might need to change the ARCH and NVCC variables. For Quevedo to use Darknet, it is also necessary to set LIBSO=1 so the linked library is built. Finally, if you want to use Darknet's data augmentation, you probably want to set OPENCV=1 to make it faster. After darknet is compiled, a binary (named darknet ) and library ( libdarknet.so in linux) will be built. Quevedo needs to know where these files are, so in the [darknet] section of the configuration, the path to the binary and library must be set. By default, these point to a darknet directory in the current directory. Some additional arguments to the darknet binary for training can be set in the options key. Network configuration \u00b6 Neural networks are ideal to deal with image data, due to their ability to find patterns and their combinations. Quevedo can help with preparing the configuration and training files to train darknet neural networks, can launch the actual training, and can compute evaluation metrics on the resulting network weights. It can also be used as a library to peruse the trained network in an application, not only for research. But no net is a silver bullet for every kind problem, and Quevedo datasets deal with different types of data with complex annotations. Therefore, Quevedo allows different network configurations to be kept in the configuration file, aiding both ensemble applications and exploration of the problem space. To add a neural network configuration to Quevedo, add a section to the config.toml file with the heading [network.<network_name>] . The initial configuration file that Quevedo creates for every dataset contains some examples that can be commented out and modified. Under this heading, different options can be set, like a subject key that gives a brief description of the purpose of the network. The most important configuration option is task , which can take the values classify or detect . New in v1.2 A key \"extend\" has been added that can be used to share network configuration. If a network net_a has a key extend = \"net_b\" , parameters from net_b will be used when no other value has been set in net_a . This can be useful to share common options when testing different networks, or to set a single source of truth for options that must be common. Since v1.3, \"extend\" is recursive, so a chain of configuration inheritance can be used. Classifier \u00b6 Classifier networks can be used with individual graphemes, and therefore use the data in the grapheme subsets of the dataset. Classify networks see the image as a whole, and try to find the best matching \"class\" from the classes they have been trained in. In Quevedo, classify networks are built with the AlexNet 1 architecture , a CNN well suited to the task. Detector \u00b6 Detector networks try to find objects in an image, and therefore are well suited for finding the different graphemes that make up a logogram. Apart from detecting the boundary boxes of the different objects, they can also do classification of the objects themselves. Depending on the nature and complexity of the data, classification of graphemes can be performed by the same network that detects them within a logogram, or can be better split into a different (or many) classifier networks. The detector network architecture used by Quevedo is YOLOv3 2 . Note After the prepare step of network use, a network configuration file is produced that can be edited to fine-tune the network architecture. Tag selection \u00b6 Since Quevedo datasets support a multi-tag annotation schema, a single \"class\"/\"label\" has to be selected for the networks in order to perform classification (including detector networks, since they have a classification step). By default the first tag of the tag schema will be used, but other tags can be selected by writing tag = \"some_tag_in_the_schema\" . A combination of the tags can be used by listing them, for example tag = [ \"some_tag\", \"some_other_tag\" ] . This will produce a single label for each grapheme by combining the values of the tags with an underscore in between, and train and evaluate the network with that single label. Annotation selection \u00b6 To specify what subsets of data to use for training and testing of a neural network, we can list the names in the subsets option. Additionally, we might want to select some logograms or graphemes to use for a particular network based on the tag values. We can do this by leaving the relevant tags for that network empty, in which case Quevedo will skip the annotation. In classify networks, finer control can also be achieved using a \"filter\" section for the network configuration. This filter accepts a key criterion which determines what tag from the annotation schema to use to select annotations. Then, an include or exclude key can be set to the list of values to filter. When include is used, if a grapheme is tagged with any of the values in the list, it is included for training and test, otherwise it is ignored. With exclude , the reverse happens. Data augmentation \u00b6 Recent versions of darknet include automatic data augmentation that happens \"on the fly\", while the network is being trained. This data augmentation is not based on semantics of the images, but on image properties like contrast or rotation. By slightly and randomly modifying the images that the network is trained on, overfitting can be avoided and better generalization achieved. Some relevant options for grapheme and logogram recognition are supported by Quevedo, and if set in the network configuration will be written into the Darknet configuration file. The header to use is [network.<network_name>.augment] , and the options supported are angle (randomly rotate images up to this amount of degrees), exposure (change brightness of the image), flip (if set to 1 , images are sometimes flipped), and, only for classify networks aspect , which modifies the grapheme width/height relation. In visual writing systems, not all of this transformations are without meaning, so by default they are disabled so that the user can choose which options make sense for their particular use case and data. Usage \u00b6 At the command line \u00b6 Once the network has been configured, the files necessary for training it can be created by running prepare . This will create a directory in the dataset, under networks , with the name of the neural network. By default, Quevedo will use the neural network marked with default = True , so to change to a different one use the option -N <network> (since this is an option common too many commands, it must be used after the quevedo binary name but before the command). Once the directory with all the files needed for training has been created, a simple invocation of train will launch the darknet executable to train the neural network. This command can be interrupted, and if enough time has passed that some partial training weights have been found, it can be later resumed by calling train again (to train from zero, use --no-resume ). The weights obtained by the training process will be stored in the network directory with the name darknet_final.weights . This is a darknet file that can be used independently of Quevedo. To evaluate the results, the test command can be used, which will get the predictions from the net for the annotations marked as \"test\" (see split ) and output some metrics, and optionally the full predictions as a csv file so that fine metrics or visualizations can be computed with something else (like R ). The predict command can be used to directly get the predictions from the neural network for some image, not necessarily one in the dataset. Since commands can be chained, a full pipeline of training and testing the net can be written as: $ quevedo -D path/to/dataset -N network_name prepare train test At the web interface \u00b6 Trained neural networks can also be used on the web interface . Networks for detection will be available for logograms, and classifier ones will be available for graphemes. They will be listed at the top right of the interface. When running them, the current annotation image will be fed to the neural network, and the predictions applied (but not saved until the user presses the save button). This can be used to visualize the neural network results, or to bootstrap manual annotation of logograms and graphemes. Example Configuration \u00b6 # Annotations for each grapheme tag_schema = [ \"COARSE\", \"FINE\", \"ALTERATION\" ] # Configuration for the darknet binary and library [darknet] path = \"darknet/darknet\" library = \"darknet/libdarknet.so\" # By passing the -mjpeg_port argument to darknet, a live image of training # progress can be seen at that port (in localhost) options = [ \"-mjpeg_port\", \"8090\" ] # Detect graphemes in logograms, and also assign a coarse-grained tag [network.logograms] subject = \"Detect and classify coarse grain graphemes in a logogram\" default = true task = \"detect\" tag = \"COARSE\" subsets = [ \"italian\", \"spanish\" ] [network.shapes] subject = \"Classify grapheme shapes\" task = \"classify\" tag = [ \"FINE\" ] subsets = [ \"simple\", \"complicated\" ] # When training grapheme classification, augment the data [network.shapes.augment] angle = 10 exposure = 0.5 # Some graphemes present alterations, annotated in the \"ALTERATION\" tag. We want # to train a specific classifier for these graphemes [network.altered] subject = \"Classify the alterations of 'complicated' graphemes\" task = \"classify\" # The label to train will be a concatenation of the \"fine\" tag and the # \"alteration\" tag = [ \"FINE\", \"ALTERATION\" ] # We have stored the graphemes with these alterations in the \"complicated\" # subset subsets = [ \"complicated\" ] # Only graphemes with the values \"multifaceted\" or \" accentuated\" for the # \"FINE\" tag will be used [network.altered.filter] criterion = \"FINE\" include = [ \"multifaceted\", \"accentuated\" ] Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (2017). \"ImageNet classification with deep convolutional neural networks\" . Communications of the ACM. 60 (6): 84\u201390. doi:10.1145/3065386. ISSN 0001-0782. S2CID 195908774. \u21a9 Redmon, Joseph and Farhadi, Ali (2018). \"YOLOv3: An Incremental Improvement\" . arXiv preprint arXiv:1804.02767. \u21a9","title":"Neural networks"},{"location":"nets/#neural-networks","text":"One of the difficulties of processing visual languages automatically is input, when it is presented in the form of images. Images are represented digitally as collections of pixels, arrayed in memory in a way that makes sense for display and storage, but which is completely disconnected to the meaning these images have to humans. Additionally, if input is hand written, graphemes can present variations which don't affect human understanding but which mean completely different pixel patterns are present. And positioning of objects is again not based on hard rules, but rather on visual interpretation. For these reasons, machine learning techniques developed in the field of computer vision are necessary to adequately process logograms and graphemes. While the researcher can use any toolkit and algorithm they prefer, Quevedo includes a module to facilitate using neural networks with Quevedo datasets.","title":"Neural networks"},{"location":"nets/#darknet","text":"Darknet is \"an open source neural network framework written in C and CUDA\", developed by the inventor of the YOLO algorithm, Joseph Redmon. This framework includes a binary and linked library which make configuring, training, and using neural networks for computer vision straightforward and efficient. The neural network module included with Quevedo needs darknet to be available. This module automatically prepares network configuration and training files from the metadata in the dataset, and can manage the training and prediction process.","title":"Darknet"},{"location":"nets/#installation","text":"We recommend using this fork by Alexey Bochkovskiy . Installation can vary depending on your environment, including the CUDA and OpenCV (optional) libraries installed, but with luck, the following will work: $ git clone https://github.com/AlexeyAB/darknet $ cd darknet <edit the Makefile> $ make In the Makefile, you probably want to enable GPU=1 and CUDNN=1 , otherwise training will be too slow. Depending on the GPU available and CUDA installation, you might need to change the ARCH and NVCC variables. For Quevedo to use Darknet, it is also necessary to set LIBSO=1 so the linked library is built. Finally, if you want to use Darknet's data augmentation, you probably want to set OPENCV=1 to make it faster. After darknet is compiled, a binary (named darknet ) and library ( libdarknet.so in linux) will be built. Quevedo needs to know where these files are, so in the [darknet] section of the configuration, the path to the binary and library must be set. By default, these point to a darknet directory in the current directory. Some additional arguments to the darknet binary for training can be set in the options key.","title":"Installation"},{"location":"nets/#network-configuration","text":"Neural networks are ideal to deal with image data, due to their ability to find patterns and their combinations. Quevedo can help with preparing the configuration and training files to train darknet neural networks, can launch the actual training, and can compute evaluation metrics on the resulting network weights. It can also be used as a library to peruse the trained network in an application, not only for research. But no net is a silver bullet for every kind problem, and Quevedo datasets deal with different types of data with complex annotations. Therefore, Quevedo allows different network configurations to be kept in the configuration file, aiding both ensemble applications and exploration of the problem space. To add a neural network configuration to Quevedo, add a section to the config.toml file with the heading [network.<network_name>] . The initial configuration file that Quevedo creates for every dataset contains some examples that can be commented out and modified. Under this heading, different options can be set, like a subject key that gives a brief description of the purpose of the network. The most important configuration option is task , which can take the values classify or detect . New in v1.2 A key \"extend\" has been added that can be used to share network configuration. If a network net_a has a key extend = \"net_b\" , parameters from net_b will be used when no other value has been set in net_a . This can be useful to share common options when testing different networks, or to set a single source of truth for options that must be common. Since v1.3, \"extend\" is recursive, so a chain of configuration inheritance can be used.","title":"Network configuration"},{"location":"nets/#classifier","text":"Classifier networks can be used with individual graphemes, and therefore use the data in the grapheme subsets of the dataset. Classify networks see the image as a whole, and try to find the best matching \"class\" from the classes they have been trained in. In Quevedo, classify networks are built with the AlexNet 1 architecture , a CNN well suited to the task.","title":"Classifier"},{"location":"nets/#detector","text":"Detector networks try to find objects in an image, and therefore are well suited for finding the different graphemes that make up a logogram. Apart from detecting the boundary boxes of the different objects, they can also do classification of the objects themselves. Depending on the nature and complexity of the data, classification of graphemes can be performed by the same network that detects them within a logogram, or can be better split into a different (or many) classifier networks. The detector network architecture used by Quevedo is YOLOv3 2 . Note After the prepare step of network use, a network configuration file is produced that can be edited to fine-tune the network architecture.","title":"Detector"},{"location":"nets/#tag-selection","text":"Since Quevedo datasets support a multi-tag annotation schema, a single \"class\"/\"label\" has to be selected for the networks in order to perform classification (including detector networks, since they have a classification step). By default the first tag of the tag schema will be used, but other tags can be selected by writing tag = \"some_tag_in_the_schema\" . A combination of the tags can be used by listing them, for example tag = [ \"some_tag\", \"some_other_tag\" ] . This will produce a single label for each grapheme by combining the values of the tags with an underscore in between, and train and evaluate the network with that single label.","title":"Tag selection"},{"location":"nets/#annotation-selection","text":"To specify what subsets of data to use for training and testing of a neural network, we can list the names in the subsets option. Additionally, we might want to select some logograms or graphemes to use for a particular network based on the tag values. We can do this by leaving the relevant tags for that network empty, in which case Quevedo will skip the annotation. In classify networks, finer control can also be achieved using a \"filter\" section for the network configuration. This filter accepts a key criterion which determines what tag from the annotation schema to use to select annotations. Then, an include or exclude key can be set to the list of values to filter. When include is used, if a grapheme is tagged with any of the values in the list, it is included for training and test, otherwise it is ignored. With exclude , the reverse happens.","title":"Annotation selection"},{"location":"nets/#data-augmentation","text":"Recent versions of darknet include automatic data augmentation that happens \"on the fly\", while the network is being trained. This data augmentation is not based on semantics of the images, but on image properties like contrast or rotation. By slightly and randomly modifying the images that the network is trained on, overfitting can be avoided and better generalization achieved. Some relevant options for grapheme and logogram recognition are supported by Quevedo, and if set in the network configuration will be written into the Darknet configuration file. The header to use is [network.<network_name>.augment] , and the options supported are angle (randomly rotate images up to this amount of degrees), exposure (change brightness of the image), flip (if set to 1 , images are sometimes flipped), and, only for classify networks aspect , which modifies the grapheme width/height relation. In visual writing systems, not all of this transformations are without meaning, so by default they are disabled so that the user can choose which options make sense for their particular use case and data.","title":"Data augmentation"},{"location":"nets/#usage","text":"","title":"Usage"},{"location":"nets/#at-the-command-line","text":"Once the network has been configured, the files necessary for training it can be created by running prepare . This will create a directory in the dataset, under networks , with the name of the neural network. By default, Quevedo will use the neural network marked with default = True , so to change to a different one use the option -N <network> (since this is an option common too many commands, it must be used after the quevedo binary name but before the command). Once the directory with all the files needed for training has been created, a simple invocation of train will launch the darknet executable to train the neural network. This command can be interrupted, and if enough time has passed that some partial training weights have been found, it can be later resumed by calling train again (to train from zero, use --no-resume ). The weights obtained by the training process will be stored in the network directory with the name darknet_final.weights . This is a darknet file that can be used independently of Quevedo. To evaluate the results, the test command can be used, which will get the predictions from the net for the annotations marked as \"test\" (see split ) and output some metrics, and optionally the full predictions as a csv file so that fine metrics or visualizations can be computed with something else (like R ). The predict command can be used to directly get the predictions from the neural network for some image, not necessarily one in the dataset. Since commands can be chained, a full pipeline of training and testing the net can be written as: $ quevedo -D path/to/dataset -N network_name prepare train test","title":"At the command line"},{"location":"nets/#at-the-web-interface","text":"Trained neural networks can also be used on the web interface . Networks for detection will be available for logograms, and classifier ones will be available for graphemes. They will be listed at the top right of the interface. When running them, the current annotation image will be fed to the neural network, and the predictions applied (but not saved until the user presses the save button). This can be used to visualize the neural network results, or to bootstrap manual annotation of logograms and graphemes.","title":"At the web interface"},{"location":"nets/#example-configuration","text":"# Annotations for each grapheme tag_schema = [ \"COARSE\", \"FINE\", \"ALTERATION\" ] # Configuration for the darknet binary and library [darknet] path = \"darknet/darknet\" library = \"darknet/libdarknet.so\" # By passing the -mjpeg_port argument to darknet, a live image of training # progress can be seen at that port (in localhost) options = [ \"-mjpeg_port\", \"8090\" ] # Detect graphemes in logograms, and also assign a coarse-grained tag [network.logograms] subject = \"Detect and classify coarse grain graphemes in a logogram\" default = true task = \"detect\" tag = \"COARSE\" subsets = [ \"italian\", \"spanish\" ] [network.shapes] subject = \"Classify grapheme shapes\" task = \"classify\" tag = [ \"FINE\" ] subsets = [ \"simple\", \"complicated\" ] # When training grapheme classification, augment the data [network.shapes.augment] angle = 10 exposure = 0.5 # Some graphemes present alterations, annotated in the \"ALTERATION\" tag. We want # to train a specific classifier for these graphemes [network.altered] subject = \"Classify the alterations of 'complicated' graphemes\" task = \"classify\" # The label to train will be a concatenation of the \"fine\" tag and the # \"alteration\" tag = [ \"FINE\", \"ALTERATION\" ] # We have stored the graphemes with these alterations in the \"complicated\" # subset subsets = [ \"complicated\" ] # Only graphemes with the values \"multifaceted\" or \" accentuated\" for the # \"FINE\" tag will be used [network.altered.filter] criterion = \"FINE\" include = [ \"multifaceted\", \"accentuated\" ] Krizhevsky, Alex; Sutskever, Ilya; Hinton, Geoffrey E. (2017). \"ImageNet classification with deep convolutional neural networks\" . Communications of the ACM. 60 (6): 84\u201390. doi:10.1145/3065386. ISSN 0001-0782. S2CID 195908774. \u21a9 Redmon, Joseph and Farhadi, Ali (2018). \"YOLOv3: An Incremental Improvement\" . arXiv preprint arXiv:1804.02767. \u21a9","title":"Example Configuration"},{"location":"pipes/","text":"Pipelines \u00b6 New in v1.2 Pipelines are new in version 1.2. Quevedo allows you to train different neural networks to recognize different objects and features. These networks can then be composed into a pipeline to build an expert system, capable of performing a bigger task than each of the networks by themselves. For example, a detection network can first locate the graphemes within a logogram, and then specialist networks used to classify each of the graphemes. When using quevedo in the command line, you can choose the pipeline to test or execute with the -P flag (equivalent to the -N flag for networks). In the web interface, the pipelines are available along the trained networks and user functions for the user to run and visualize. Note Quevedo pipelines can be used to create expert systems for processing visually complex images. One such example, for the case of SignWriting, is described in the article Automatic SignWriting Recognition . Pipeline configuration \u00b6 Pipelines are added to the config.toml file of the dataset. Each entry is the name of the pipeline, added to the \"pipeline\" table. There are a number of types of pipelines that can be used, each with their own configuration, but there are some common options. Common options \u00b6 Options for the pipelines often take as value the name of a neural network. Most of the time, instead of a neural network, the name of a pipeline can be used. Quevedo will search the configuration file for pipelines or networks with a matching name, so neither \"network\" nor \"pipeline\" need to be prepended. If a \"subsets\" key is given to a pipeline, you will be able to use the test command to evaluate the performance of the pipeline on those sets. Testing pipelines follows the same rules as for networks, and uses the configured test folds. Training full pipelines is not yet supported, please train the networks individually. Pipelines can also use the extend keyword to inherit configuration from another pipeline, making it easy to set the same subsets to test all pipelines, or share some of the options. This extension is recursive, so chains of pipeline configurations can be used. Logogram recognizer \u00b6 A logogram recognizer pipeline has two steps. The first step uses a detector network to find graphemes in an image. An optional second step uses a classifier network or another pipeline to then extract the tags for each of the graphemes. The end result is the detected logogram but with the graphemes augmented by the classifier. # Example logogram pipeline. It uses a network named \"detector\" to find the # graphemes, and then further classifies them with a network named \"classifier\" [pipeline.logograms] detect = \"detector\" classify = \"classifier\" Sequence classifier \u00b6 A sequence classifier uses many classifier networks or pipelines to iteratively augment the annotation of a grapheme. It has one single option, sequence , which is a list of the sub-systems to run. Additionally to networks and pipelines, the steps in the sequence can be lambda functions to run on the grapheme, or longer functions defined in a user script. For this, place the script in the scripts directory, and use as step script_name.py:function_name . # Example sequence pipeline. It uses a network called \"classifier1\" to find # a first possible set of tags for the grapheme. Then, a function # \"error_correction\" in the \"functions.py\" user script fixes some common errors. # A second network called \"classifier2\" makes use of the fixed grapheme to get # a better prediction. [pipeline.sequence] extend = \"defaults\" sequence = [ \"classifier1\" \"funtions.py:error_correction\", \"classifier2\" ] Branching classifier \u00b6 A branching pipeline can serve to classify graphemes using different networks or pipelines according to some of the grapheme characteristics. A criterion option sets the value to use to choose the branch, and then the other options are the networks or sub-pipelines for each branch. The criterion can be the name of a tag, or a lambda function to call on the grapheme. # Example branching pipeline [pipeline.branching] criterion = \"lambda g: g.tags.get('TAG1')\" criterion = \"TAG1\" # equivalent to the previous one value1 = \"classifier_for_value1s\" value2 = \"classifier_for_value2s\"","title":"Pipelines"},{"location":"pipes/#pipelines","text":"New in v1.2 Pipelines are new in version 1.2. Quevedo allows you to train different neural networks to recognize different objects and features. These networks can then be composed into a pipeline to build an expert system, capable of performing a bigger task than each of the networks by themselves. For example, a detection network can first locate the graphemes within a logogram, and then specialist networks used to classify each of the graphemes. When using quevedo in the command line, you can choose the pipeline to test or execute with the -P flag (equivalent to the -N flag for networks). In the web interface, the pipelines are available along the trained networks and user functions for the user to run and visualize. Note Quevedo pipelines can be used to create expert systems for processing visually complex images. One such example, for the case of SignWriting, is described in the article Automatic SignWriting Recognition .","title":"Pipelines"},{"location":"pipes/#pipeline-configuration","text":"Pipelines are added to the config.toml file of the dataset. Each entry is the name of the pipeline, added to the \"pipeline\" table. There are a number of types of pipelines that can be used, each with their own configuration, but there are some common options.","title":"Pipeline configuration"},{"location":"pipes/#common-options","text":"Options for the pipelines often take as value the name of a neural network. Most of the time, instead of a neural network, the name of a pipeline can be used. Quevedo will search the configuration file for pipelines or networks with a matching name, so neither \"network\" nor \"pipeline\" need to be prepended. If a \"subsets\" key is given to a pipeline, you will be able to use the test command to evaluate the performance of the pipeline on those sets. Testing pipelines follows the same rules as for networks, and uses the configured test folds. Training full pipelines is not yet supported, please train the networks individually. Pipelines can also use the extend keyword to inherit configuration from another pipeline, making it easy to set the same subsets to test all pipelines, or share some of the options. This extension is recursive, so chains of pipeline configurations can be used.","title":"Common options"},{"location":"pipes/#logogram-recognizer","text":"A logogram recognizer pipeline has two steps. The first step uses a detector network to find graphemes in an image. An optional second step uses a classifier network or another pipeline to then extract the tags for each of the graphemes. The end result is the detected logogram but with the graphemes augmented by the classifier. # Example logogram pipeline. It uses a network named \"detector\" to find the # graphemes, and then further classifies them with a network named \"classifier\" [pipeline.logograms] detect = \"detector\" classify = \"classifier\"","title":"Logogram recognizer"},{"location":"pipes/#sequence-classifier","text":"A sequence classifier uses many classifier networks or pipelines to iteratively augment the annotation of a grapheme. It has one single option, sequence , which is a list of the sub-systems to run. Additionally to networks and pipelines, the steps in the sequence can be lambda functions to run on the grapheme, or longer functions defined in a user script. For this, place the script in the scripts directory, and use as step script_name.py:function_name . # Example sequence pipeline. It uses a network called \"classifier1\" to find # a first possible set of tags for the grapheme. Then, a function # \"error_correction\" in the \"functions.py\" user script fixes some common errors. # A second network called \"classifier2\" makes use of the fixed grapheme to get # a better prediction. [pipeline.sequence] extend = \"defaults\" sequence = [ \"classifier1\" \"funtions.py:error_correction\", \"classifier2\" ]","title":"Sequence classifier"},{"location":"pipes/#branching-classifier","text":"A branching pipeline can serve to classify graphemes using different networks or pipelines according to some of the grapheme characteristics. A criterion option sets the value to use to choose the branch, and then the other options are the networks or sub-pipelines for each branch. The criterion can be the name of a tag, or a lambda function to call on the grapheme. # Example branching pipeline [pipeline.branching] criterion = \"lambda g: g.tags.get('TAG1')\" criterion = \"TAG1\" # equivalent to the previous one value1 = \"classifier_for_value1s\" value2 = \"classifier_for_value2s\"","title":"Branching classifier"},{"location":"web/","text":"Web Interface \u00b6 The data that Quevedo aims to manage are highly visual, and therefore a visual interface can be very useful. Indeed, in the case of annotation, being able to see the target of annotation is fundamental. It is also a task which may be shared between a team, or conducted by people who are not data scientists or engineers and don't feel comfortable with code and a command line interface. Quevedo provides a web interface which can be used to visualize, manage and annotate data in a Quevedo dataset. The web interface has the advantage that is graphical, and it can also be run on a server or some shared computer accesible via the internet so that collaborators can work on the dataset without any infrastructure needs on their part (beyond a modern browser). To use the web interface locally, just run quevedo web . This will launch the server in a local port and open a browser window at the appropriate location. To quit the server, just press Ctrl+C in the terminal window. For more options see the rest of this document, and for usage of the web interface see here . Note The web interface for Quevedo is not intended to be used as a permanent, production website, but rather to provide access to collaborators during a project. Configuration \u00b6 Some options for the web interface can be configured in the dataset configuration file . Since some of these settings may be sensitive, they can be set in the local configuration file if the dataset is going to be distributed publicly. The main configuration is set under the heading web , and the following options can be set: Server options \u00b6 host , port : IP address and port to bind the server to. mount_path : path under which the application will be mounted. secret_key : secret string to sign session cookies. You can generate a random one for your installation with python -c 'from secrets import token_hex; print(token_hex(16))' . Interface options \u00b6 lang : the language for messages in the web interface. Supported values for now are en (english) or es (spanish). colors : a custom list of colors (in hex notation) can be given for the logogram annotation interface to use. For example: colors = ['#ff0000', '#00ff00', '#0000ff'] . public : If true, no login will be required. If false, access will only be provided to logged in users. Users \u00b6 To create a user (needed if the dataset interface is not set to public ) add a heading [web.users.<user_name>] , with the following options: password : hex digest of the sha1 hash of the password for the user. You can generate the hash with the following code: python -c 'import hashlib; print(hashlib.new(\"sha1\", \"thepassword\".encode(\"utf8\")).hexdigest());' . read : subsets that the user has read access to. Can be ALL , NONE , or a list of subsets to allow access to. These strings are actually regexes , so any subset matching them will be available. write : subsets that the user has write access to. Follows the same syntax of read . Write access means adding annotations to a subset, and saving modified annotations to the server (the annotation can be changed locally, but not saved). Assuming we have the sets spanish_a , spanish_b and english_a : read = [ \"spanish\", \"english\" ] will allow access to all sets. read = [ \"spanish_a\" ] will only allow access to the spanish_a set. read = [ \"_a\" ] will allow access to the spanish_a and english_a sets. You can use ^ , $ , and other regex syntax to be more specific. Warning This system of managing users is not very sophisticated, intended to prevent mistakes or unintentional leaks of data rather than actual security. Please don't put here your email or bank passwords, nor ask your collaborators to provide them to you.","title":"Web Interface"},{"location":"web/#web-interface","text":"The data that Quevedo aims to manage are highly visual, and therefore a visual interface can be very useful. Indeed, in the case of annotation, being able to see the target of annotation is fundamental. It is also a task which may be shared between a team, or conducted by people who are not data scientists or engineers and don't feel comfortable with code and a command line interface. Quevedo provides a web interface which can be used to visualize, manage and annotate data in a Quevedo dataset. The web interface has the advantage that is graphical, and it can also be run on a server or some shared computer accesible via the internet so that collaborators can work on the dataset without any infrastructure needs on their part (beyond a modern browser). To use the web interface locally, just run quevedo web . This will launch the server in a local port and open a browser window at the appropriate location. To quit the server, just press Ctrl+C in the terminal window. For more options see the rest of this document, and for usage of the web interface see here . Note The web interface for Quevedo is not intended to be used as a permanent, production website, but rather to provide access to collaborators during a project.","title":"Web Interface"},{"location":"web/#configuration","text":"Some options for the web interface can be configured in the dataset configuration file . Since some of these settings may be sensitive, they can be set in the local configuration file if the dataset is going to be distributed publicly. The main configuration is set under the heading web , and the following options can be set:","title":"Configuration"},{"location":"web/#server-options","text":"host , port : IP address and port to bind the server to. mount_path : path under which the application will be mounted. secret_key : secret string to sign session cookies. You can generate a random one for your installation with python -c 'from secrets import token_hex; print(token_hex(16))' .","title":"Server options"},{"location":"web/#interface-options","text":"lang : the language for messages in the web interface. Supported values for now are en (english) or es (spanish). colors : a custom list of colors (in hex notation) can be given for the logogram annotation interface to use. For example: colors = ['#ff0000', '#00ff00', '#0000ff'] . public : If true, no login will be required. If false, access will only be provided to logged in users.","title":"Interface options"},{"location":"web/#users","text":"To create a user (needed if the dataset interface is not set to public ) add a heading [web.users.<user_name>] , with the following options: password : hex digest of the sha1 hash of the password for the user. You can generate the hash with the following code: python -c 'import hashlib; print(hashlib.new(\"sha1\", \"thepassword\".encode(\"utf8\")).hexdigest());' . read : subsets that the user has read access to. Can be ALL , NONE , or a list of subsets to allow access to. These strings are actually regexes , so any subset matching them will be available. write : subsets that the user has write access to. Follows the same syntax of read . Write access means adding annotations to a subset, and saving modified annotations to the server (the annotation can be changed locally, but not saved). Assuming we have the sets spanish_a , spanish_b and english_a : read = [ \"spanish\", \"english\" ] will allow access to all sets. read = [ \"spanish_a\" ] will only allow access to the spanish_a set. read = [ \"_a\" ] will allow access to the spanish_a and english_a sets. You can use ^ , $ , and other regex syntax to be more specific. Warning This system of managing users is not very sophisticated, intended to prevent mistakes or unintentional leaks of data rather than actual security. Please don't put here your email or bank passwords, nor ask your collaborators to provide them to you.","title":"Users"},{"location":"web_use/","text":"Using the web interface \u00b6 Dataset overview \u00b6 The main page of Quevedo's web interface presents an overview of the dataset, with the different subsets of data listed as folders. On top, general dataset information (from the configuration file) is listed. Data subsets are divided into logogram and grapheme sets, and the number of images in each is noted. Icons are actually Unicode Emojis, so they may differ between platforms. To browse a subset, click on the folder icon. A new (empty) subset can be created with the \"Create new\" button. Subset listing \u00b6 Each subset has its own listing, where the images contained can be quickly previewed. To go back to the general overview, click on the dataset title on the top left or on the \"back to list\" icon. At the end of the listing, an \"Upload new\" button allows adding images to the dataset (for a quicker way, see the command add_images . Clicking on an image, or the \"edit\" icon underneath, takes you to the annotation page. Grapheme annotation \u00b6 The grapheme annotation page shows, under the heading \"Annotation\", the image to annotate to the left, and the tags to the right. The tag headers are the ones set in the dataset configuration file under the option tag_schema , and the values are to be input by the user. On top of this, the metadata associated with this annotation can be edited. Below the annotation, any quick guide text set for annotators in the option annotation_help is displayed. The header contains a number of buttons for navigation and access to annotation functions. The links allow you to navigate up to the overview or subset listing, and the arrows after the annotation id navigate to the previous or next annotation. There is also an undo button that lets you revert any changes from this session (even after saving, but not after navigating away). The save icon sends your changes to the server to be stored in the dataset. In the top right, a list of functions can be selected, and then run using the gears button. The functions will do some transformation on the annotation, and send it to you to be previewed. If the changes are OK, you can click the save button to store them permanently. Note If you are a collaborator, please ask the lead of the project what each of these functions do and when to use them. The functions available are of two kinds: Trained networks \u00b6 When editing graphemes, any neural networks which have been already trained and which have the task classify will be listed. When run, the annotation image will be fed to the neural network, the prediction decoded, and the new tags sent to the web interface. This can be used to both visually check the networks, and to bootstrap manual annotation by using the networks output as a first step. User scripts \u00b6 Any user script under the scripts directory which has a filename starting with grapheme will be listed and available for collaborators to use. Please note that any code in the script will be able to be run by collaborators, so if there are potentially dangerous operations or modifications in the script, don't make it available in the web interface (by changing the name) or properly advise your collaborators. Logogram annotation \u00b6 The page for logogram annotation is very similar to the grapheme annotation page, but the actual annotation area is more complicated. On the right, instead of a list of tags, there is a table. Each row corresponds to a different grapheme found within the logogram, and the columns are the tags from the tag_schema . You can move right and left with Tab and Shift+Tab , and up and down with the arrow keys in the keyboard. To add a grapheme, click on the logogram image and drag the appearing rectangle until it covers the full grapheme area. You can redo the rectangle while this grapheme is selected, and you can always modify the rectangle for a grapheme selecting it again by clicking on its row. To finish the rectangle and deselect, click outside of the logogram image or press Enter . You can remove graphemes by clicking on the Trash button at the right of each row. You can also change the color of the grapheme rectangle with the button on the left of each row, but please note that these colors are not stored by Quevedo, and are only a visual aid to annotation while on the web interface. As with graphemes before, changes can be undone, and must be sent to the server to be stored with the Save button. The networks offered in the functions list on the top right will be those which have the task detect , and the scripts available will be those with filenames starting with logogram .","title":"Using the web interface"},{"location":"web_use/#using-the-web-interface","text":"","title":"Using the web interface"},{"location":"web_use/#dataset-overview","text":"The main page of Quevedo's web interface presents an overview of the dataset, with the different subsets of data listed as folders. On top, general dataset information (from the configuration file) is listed. Data subsets are divided into logogram and grapheme sets, and the number of images in each is noted. Icons are actually Unicode Emojis, so they may differ between platforms. To browse a subset, click on the folder icon. A new (empty) subset can be created with the \"Create new\" button.","title":"Dataset overview"},{"location":"web_use/#subset-listing","text":"Each subset has its own listing, where the images contained can be quickly previewed. To go back to the general overview, click on the dataset title on the top left or on the \"back to list\" icon. At the end of the listing, an \"Upload new\" button allows adding images to the dataset (for a quicker way, see the command add_images . Clicking on an image, or the \"edit\" icon underneath, takes you to the annotation page.","title":"Subset listing"},{"location":"web_use/#grapheme-annotation","text":"The grapheme annotation page shows, under the heading \"Annotation\", the image to annotate to the left, and the tags to the right. The tag headers are the ones set in the dataset configuration file under the option tag_schema , and the values are to be input by the user. On top of this, the metadata associated with this annotation can be edited. Below the annotation, any quick guide text set for annotators in the option annotation_help is displayed. The header contains a number of buttons for navigation and access to annotation functions. The links allow you to navigate up to the overview or subset listing, and the arrows after the annotation id navigate to the previous or next annotation. There is also an undo button that lets you revert any changes from this session (even after saving, but not after navigating away). The save icon sends your changes to the server to be stored in the dataset. In the top right, a list of functions can be selected, and then run using the gears button. The functions will do some transformation on the annotation, and send it to you to be previewed. If the changes are OK, you can click the save button to store them permanently. Note If you are a collaborator, please ask the lead of the project what each of these functions do and when to use them. The functions available are of two kinds:","title":"Grapheme annotation"},{"location":"web_use/#trained-networks","text":"When editing graphemes, any neural networks which have been already trained and which have the task classify will be listed. When run, the annotation image will be fed to the neural network, the prediction decoded, and the new tags sent to the web interface. This can be used to both visually check the networks, and to bootstrap manual annotation by using the networks output as a first step.","title":"Trained networks"},{"location":"web_use/#user-scripts","text":"Any user script under the scripts directory which has a filename starting with grapheme will be listed and available for collaborators to use. Please note that any code in the script will be able to be run by collaborators, so if there are potentially dangerous operations or modifications in the script, don't make it available in the web interface (by changing the name) or properly advise your collaborators.","title":"User scripts"},{"location":"web_use/#logogram-annotation","text":"The page for logogram annotation is very similar to the grapheme annotation page, but the actual annotation area is more complicated. On the right, instead of a list of tags, there is a table. Each row corresponds to a different grapheme found within the logogram, and the columns are the tags from the tag_schema . You can move right and left with Tab and Shift+Tab , and up and down with the arrow keys in the keyboard. To add a grapheme, click on the logogram image and drag the appearing rectangle until it covers the full grapheme area. You can redo the rectangle while this grapheme is selected, and you can always modify the rectangle for a grapheme selecting it again by clicking on its row. To finish the rectangle and deselect, click outside of the logogram image or press Enter . You can remove graphemes by clicking on the Trash button at the right of each row. You can also change the color of the grapheme rectangle with the button on the left of each row, but please note that these colors are not stored by Quevedo, and are only a visual aid to annotation while on the web interface. As with graphemes before, changes can be undone, and must be sent to the server to be stored with the Save button. The networks offered in the functions list on the top right will be those which have the task detect , and the scripts available will be those with filenames starting with logogram .","title":"Logogram annotation"}]}