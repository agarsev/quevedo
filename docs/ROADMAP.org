#+title: Roadmap

Don't remove elements from here until version released, then use this file to
populate the changelog.

* Next

Next version: 1.1

** TODO Dataset version 1 (not compatible with 0)

*** DONE Dataset versioning
    CLOSED: [2021-08-03 Tue 12:57]
Version field in config.toml, Dataset config.toml without version is version 0

*** DONE migration command
    CLOSED: [2021-08-03 Tue 12:57]

*** DONE Dict of tags
    CLOSED: [2021-08-04 Wed 13:03]
Move tags from a list to a (default)dict

- [X] Migrate json files
- [X] Grapheme
- [X] BoundGrapheme
- [X] CLI (generate, dataset, extract)
- [X] Web interface
- [X] Nets

*** TODO Test & Doc

- [X] Test with real corpus
- [ ] Documentation
- [ ] Fix Examples

** TODO Improve train/test set

Also for dataset version 1.

Instead of train/test "sets", now a "fold" value for annotations. `split`
command assigns this value (a number) randomly. The fold is tracked through
extract and generate commands.

Train and test sets are configured in config.toml as sets of folds to include in
each. Changing this can be useful for k-fold cross-validation. Also the
proportions of training and test annotations can be changed without having to
modify annotations or re-run processess. Finally, some folds can be held out,
not included in either train or test.

This is better than "lists" of annotations because we need to keep track where
extracted graphemes come from, and generated logograms, so test sets are not
cross-contaminated.

*** DONE Change "set" to "fold" in annotation
    CLOSED: [2021-09-11 Sat 19:31]

- [X] Code
- [X] Migration

*** DONE Fix split command
    CLOSED: [2021-09-11 Sat 20:03]

*** DONE Use folds
    CLOSED: [2021-09-13 Mon 15:36]

- [X] config.toml
- [X] default_config
- [X] migration of config
- [X] is_train/is_test methods, networks use this
- [X] web app

*** DONE Conserve folds
    CLOSED: [2021-09-13 Mon 18:59]

- [X] extract
- [X] generate

*** TODO Test and doc

- [X] Test correct numbers
- [X] Test correct train/test usage
- [ ] Test conserve
- [ ] Doc

** TODO Net config improvements
*** TODO Configure max epochs, size, etc. More params in general.
*** TODO Improve multi tags
Joining with underscore problem if tags have underscores. Maybe format strings?
Choose joining char? Use exotic unicode?

** TODO Web interface improvements

*** DONE Allow using touch in mobile devices

*** TODO Filter/search annotations in listing according to some tag(s).

*** TODO Autosuggest values for tagging
maybe use https://developer.mozilla.org/en-US/docs/Web/HTML/Element/datalist

*** TODO Improve messaging
Remove "saved" message when doing changes (inconsistent), maybe load indicator

** TODO Annotation flags

Instead of a "check" for annotated/not annotated, custom "flags" in config.toml
that are checkboxes in meta and can be toggled in web interface.

** Other

*** DONE Memoize networks
    CLOSED: [2021-09-13 Mon 21:36]

`dataset.get_network` stores the network, so you can call it as many times as
you want without memory leaks and no overhead, no need to memoize the `Network`
yourself.

* Backlog

- [ ] Try again to do train-time testing with darknet to see when overfitting
    starts.
- [ ] When scripts modify images, don't save them, but return that it has been
    modified (ie return modified_tags, modified_img) and then it is `run_script`
    that saves the image to the appropriate path. Coversely, in the web
    interface the updated image can be sent to the frontend to be previewed, and
    if they want to save it send it back to the server on "save". The
    complication is that the image is now frontend state, not just a src link.
- [ ] Allow deleting entries in web (just move the last to the hole). Maybe
    add `delete` in cli too?
- [ ] Mobile interface for the web app. Maybe integrate with camera/scan app,
    make quevedo a target for "sharing" (uploading) images.
- [ ] Web user improvements: groups and recording annotator in json.
- [ ] Improve nets. Try again with grayscale images now that we use AlexeyAB
    fork, check letterboxing, try different configs, etc. Some of this can be
    done by improving the python code that access the darknet dll (eg the
    channels) and some might be better to do ourselves (eg letterboxing and
    resizing). Maybe migrate to tensorflow and keras if we get hardware where to
    test it.
- [ ] Add the possibility to the split command to do homogeneous splits taking
    into account annotation values, so that sparse classes are not removed from
    the training set.

